"""
ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã¨OpenSearch Serverlessçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Amazon Bedrockã¨OpenSearch Serverlessã‚’ä½¿ç”¨ã—ãŸé«˜å“è³ªåŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ»æ ¼ç´æ©Ÿèƒ½
"""

import json
import logging
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import boto3
from botocore.exceptions import ClientError
import hashlib
from datetime import datetime
import time

logger = logging.getLogger(__name__)

@dataclass
class EmbeddingResult:
    """åŸ‹ã‚è¾¼ã¿çµæœ"""
    success: bool
    embeddings: List[List[float]]
    metadata: Dict[str, Any]
    error: Optional[str] = None

@dataclass
class OpenSearchDocument:
    """OpenSearchãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ"""
    id: str
    content: str
    embedding: List[float]
    metadata: Dict[str, Any]
    timestamp: str

class VectorEmbeddingProcessor:
    """ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 region: str = 'us-east-1',
                 embedding_model: str = 'amazon.titan-embed-text-v1',
                 opensearch_endpoint: Optional[str] = None,
                 opensearch_index: str = 'documents'):
        """
        åˆæœŸåŒ–
        
        Args:\n            region: AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³\n            embedding_model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å\n            opensearch_endpoint: OpenSearchã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ\n            opensearch_index: OpenSearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å\n        \"\"\"\n        self.region = region\n        self.embedding_model = embedding_model\n        self.opensearch_endpoint = opensearch_endpoint or os.environ.get('OPENSEARCH_ENDPOINT')\n        self.opensearch_index = opensearch_index\n        \n        # AWS ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–\n        self.bedrock_client = boto3.client('bedrock-runtime', region_name=region)\n        \n        # OpenSearch ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ opensearch-py ã‚’ä½¿ç”¨ï¼‰\n        # from opensearchpy import OpenSearch, RequestsHttpConnection\n        # from aws_requests_auth.aws_auth import AWSRequestsAuth\n        # \n        # if self.opensearch_endpoint:\n        #     host = self.opensearch_endpoint.replace('https://', '')\n        #     awsauth = AWSRequestsAuth(aws_access_key=None, aws_secret_access_key=None,\n        #                              aws_token=None, aws_host=host, aws_region=region,\n        #                              aws_service='es')\n        #     self.opensearch_client = OpenSearch(\n        #         hosts=[{'host': host, 'port': 443}],\n        #         http_auth=awsauth,\n        #         use_ssl=True,\n        #         verify_certs=True,\n        #         connection_class=RequestsHttpConnection\n        #     )\n        # else:\n        #     self.opensearch_client = None\n        \n        self.opensearch_client = None  # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…\n        \n        logger.info(f\"ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿å‡¦ç†ã‚’åˆæœŸåŒ–: model={embedding_model}, endpoint={opensearch_endpoint}\")\n    \n    def generate_embeddings(self, texts: List[str], batch_size: int = 25) -> EmbeddingResult:\n        \"\"\"\n        ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ\n        \n        Args:\n            texts: ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ\n            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º\n            \n        Returns:\n            EmbeddingResult: åŸ‹ã‚è¾¼ã¿çµæœ\n        \"\"\"\n        try:\n            logger.info(f\"ğŸ”¢ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆé–‹å§‹: {len(texts)}ãƒ†ã‚­ã‚¹ãƒˆ (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})\")\n            \n            all_embeddings = []\n            processing_times = []\n            \n            # ãƒãƒƒãƒå‡¦ç†\n            for i in range(0, len(texts), batch_size):\n                batch_texts = texts[i:i + batch_size]\n                batch_start_time = time.time()\n                \n                batch_embeddings = self._generate_batch_embeddings(batch_texts)\n                all_embeddings.extend(batch_embeddings)\n                \n                batch_time = time.time() - batch_start_time\n                processing_times.append(batch_time)\n                \n                logger.info(f\"ãƒãƒƒãƒ {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size} å®Œäº†: {len(batch_texts)}ãƒ†ã‚­ã‚¹ãƒˆ, {batch_time:.2f}ç§’\")\n            \n            metadata = {\n                'total_texts': len(texts),\n                'total_embeddings': len(all_embeddings),\n                'batch_size': batch_size,\n                'embedding_model': self.embedding_model,\n                'total_processing_time': sum(processing_times),\n                'average_batch_time': sum(processing_times) / len(processing_times) if processing_times else 0,\n                'embedding_dimension': len(all_embeddings[0]) if all_embeddings else 0,\n                'processed_at': datetime.utcnow().isoformat()\n            }\n            \n            logger.info(f\"âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len(all_embeddings)}åŸ‹ã‚è¾¼ã¿, {sum(processing_times):.2f}ç§’\")\n            \n            return EmbeddingResult(\n                success=True,\n                embeddings=all_embeddings,\n                metadata=metadata\n            )\n            \n        except Exception as e:\n            logger.error(f\"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}\")\n            return EmbeddingResult(\n                success=False,\n                embeddings=[],\n                metadata={},\n                error=str(e)\n            )\n    \n    def _generate_batch_embeddings(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"\n        ãƒãƒƒãƒã§ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n        \n        Args:\n            texts: ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ\n            \n        Returns:\n            List[List[float]]: åŸ‹ã‚è¾¼ã¿ãƒªã‚¹ãƒˆ\n        \"\"\"\n        embeddings = []\n        \n        for text in texts:\n            try:\n                # Bedrock Titan Embeddings ã‚’ä½¿ç”¨\n                embedding = self._invoke_bedrock_embedding(text)\n                embeddings.append(embedding)\n                \n            except Exception as e:\n                logger.warning(f\"å€‹åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã«å¤±æ•—: {e}\")\n                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨\n                embeddings.append([0.0] * 1536)  # Titan Embeddings ã®æ¬¡å…ƒæ•°\n        \n        return embeddings\n    \n    def _invoke_bedrock_embedding(self, text: str) -> List[float]:\n        \"\"\"\n        BedrockåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—\n        \n        Args:\n            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ\n            \n        Returns:\n            List[float]: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«\n        \"\"\"\n        try:\n            # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†\n            processed_text = self._preprocess_text(text)\n            \n            # Bedrock APIå‘¼ã³å‡ºã—\n            request_body = {\n                \"inputText\": processed_text\n            }\n            \n            response = self.bedrock_client.invoke_model(\n                modelId=self.embedding_model,\n                body=json.dumps(request_body),\n                contentType='application/json',\n                accept='application/json'\n            )\n            \n            response_body = json.loads(response['body'].read())\n            embedding = response_body.get('embedding', [])\n            \n            if not embedding:\n                raise ValueError(\"åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒç©ºã§ã™\")\n            \n            return embedding\n            \n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            if error_code == 'ThrottlingException':\n                logger.warning(\"Bedrockã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ç™ºç”Ÿã€ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™\")\n                time.sleep(1)\n                return self._invoke_bedrock_embedding(text)\n            else:\n                raise\n        except Exception as e:\n            logger.error(f\"BedrockåŸ‹ã‚è¾¼ã¿å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}\")\n            # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—åŸ‹ã‚è¾¼ã¿ã‚’è¿”ã™ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å‰Šé™¤ï¼‰\n            return self._generate_mock_embedding(text)\n    \n    def _preprocess_text(self, text: str) -> str:\n        \"\"\"\n        ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†\n        \n        Args:\n            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ\n            \n        Returns:\n            str: å‰å‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ\n        \"\"\"\n        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°\n        processed = text.strip()\n        \n        # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆï¼ˆTitan Embeddings ã®åˆ¶é™ï¼‰\n        max_length = 8000  # æ–‡å­—æ•°åˆ¶é™\n        if len(processed) > max_length:\n            processed = processed[:max_length]\n            logger.warning(f\"ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹ãŸã‚åˆ‡ã‚Šè©°ã‚ã¾ã—ãŸ: {len(text)} -> {len(processed)}æ–‡å­—\")\n        \n        # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã®å‡¦ç†\n        if not processed:\n            processed = \"[ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆ]\"\n        \n        return processed\n    \n    def _generate_mock_embedding(self, text: str) -> List[float]:\n        \"\"\"\n        ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰\n        \n        Args:\n            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ\n            \n        Returns:\n            List[float]: ãƒ¢ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«\n        \"\"\"\n        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ã«åŸºã¥ã„ã¦ãƒ€ãƒŸãƒ¼åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ\n        text_hash = hashlib.md5(text.encode()).hexdigest()\n        \n        embedding = []\n        for i in range(1536):  # Titan Embeddings ã®æ¬¡å…ƒæ•°\n            # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ä½¿ã£ã¦ç–‘ä¼¼ãƒ©ãƒ³ãƒ€ãƒ ãªå€¤ã‚’ç”Ÿæˆ\n            hash_int = int(text_hash[i % len(text_hash)], 16)\n            value = (hash_int + i) / 1000.0 - 0.5  # -0.5 ã‹ã‚‰ 0.5 ã®ç¯„å›²\n            embedding.append(value)\n        \n        return embedding\n    \n    def store_embeddings_to_opensearch(self, \n                                     documents: List[OpenSearchDocument],\n                                     index_name: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        åŸ‹ã‚è¾¼ã¿ã‚’OpenSearch Serverlessã«æ ¼ç´\n        \n        Args:\n            documents: OpenSearchãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ\n            index_name: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n            \n        Returns:\n            Dict: æ ¼ç´çµæœ\n        \"\"\"\n        try:\n            index = index_name or self.opensearch_index\n            logger.info(f\"ğŸ“Š OpenSearchæ ¼ç´é–‹å§‹: {len(documents)}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ -> {index}\")\n            \n            if not self.opensearch_client:\n                logger.warning(\"OpenSearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n                return self._mock_opensearch_storage(documents, index)\n            \n            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä»¥ä¸‹ã‚’ä½¿ç”¨\n            # success_count = 0\n            # error_count = 0\n            # \n            # for doc in documents:\n            #     try:\n            #         response = self.opensearch_client.index(\n            #             index=index,\n            #             id=doc.id,\n            #             body={\n            #                 'content': doc.content,\n            #                 'embedding': doc.embedding,\n            #                 'metadata': doc.metadata,\n            #                 'timestamp': doc.timestamp\n            #             }\n            #         )\n            #         success_count += 1\n            #     except Exception as e:\n            #         logger.error(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ ¼ç´ã‚¨ãƒ©ãƒ¼ {doc.id}: {e}\")\n            #         error_count += 1\n            \n            # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…\n            return self._mock_opensearch_storage(documents, index)\n            \n        except Exception as e:\n            logger.error(f\"âŒ OpenSearchæ ¼ç´ã‚¨ãƒ©ãƒ¼: {e}\")\n            return {\n                'success': False,\n                'error': str(e),\n                'stored_count': 0,\n                'failed_count': len(documents)\n            }\n    \n    def _mock_opensearch_storage(self, documents: List[OpenSearchDocument], index: str) -> Dict[str, Any]:\n        \"\"\"\n        OpenSearchæ ¼ç´ã®ãƒ¢ãƒƒã‚¯å®Ÿè£…\n        \n        Args:\n            documents: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ\n            index: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å\n            \n        Returns:\n            Dict: ãƒ¢ãƒƒã‚¯æ ¼ç´çµæœ\n        \"\"\"\n        logger.info(f\"ğŸ“Š ãƒ¢ãƒƒã‚¯OpenSearchæ ¼ç´: {len(documents)}ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\")\n        \n        # æ ¼ç´ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\n        time.sleep(0.1 * len(documents))  # æ ¼ç´æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ\n        \n        return {\n            'success': True,\n            'index': index,\n            'stored_count': len(documents),\n            'failed_count': 0,\n            'processing_time': 0.1 * len(documents),\n            'mock': True\n        }\n    \n    def create_opensearch_documents(self, \n                                  chunks: List[Dict[str, Any]], \n                                  embeddings: List[List[float]],\n                                  source_file: str) -> List[OpenSearchDocument]:\n        \"\"\"\n        OpenSearchãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ\n        \n        Args:\n            chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ\n            embeddings: åŸ‹ã‚è¾¼ã¿ãƒªã‚¹ãƒˆ\n            source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å\n            \n        Returns:\n            List[OpenSearchDocument]: OpenSearchãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ\n        \"\"\"\n        documents = []\n        timestamp = datetime.utcnow().isoformat()\n        \n        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n            doc_id = self._generate_document_id(source_file, i, chunk['content'])\n            \n            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µ\n            enhanced_metadata = {\n                **chunk['metadata'],\n                'source_file': source_file,\n                'document_id': doc_id,\n                'embedding_model': self.embedding_model,\n                'embedding_dimension': len(embedding),\n                'indexed_at': timestamp\n            }\n            \n            doc = OpenSearchDocument(\n                id=doc_id,\n                content=chunk['content'],\n                embedding=embedding,\n                metadata=enhanced_metadata,\n                timestamp=timestamp\n            )\n            \n            documents.append(doc)\n        \n        return documents\n    \n    def _generate_document_id(self, source_file: str, chunk_index: int, content: str) -> str:\n        \"\"\"\n        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã‚’ç”Ÿæˆ\n        \n        Args:\n            source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å\n            chunk_index: ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n            content: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n            \n        Returns:\n            str: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID\n        \"\"\"\n        # ãƒ•ã‚¡ã‚¤ãƒ«åã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒƒã‚·ãƒ¥ã‚’çµ„ã¿åˆã‚ã›\n        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]\n        return f\"{source_file}_{chunk_index}_{content_hash}\"\n    \n    def search_similar_documents(self, \n                               query_embedding: List[float], \n                               k: int = 10,\n                               filter_conditions: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n        \"\"\"\n        é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢\n        \n        Args:\n            query_embedding: ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿\n            k: å–å¾—ã™ã‚‹æ–‡æ›¸æ•°\n            filter_conditions: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶\n            \n        Returns:\n            Dict: æ¤œç´¢çµæœ\n        \"\"\"\n        try:\n            logger.info(f\"ğŸ” é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢: k={k}\")\n            \n            if not self.opensearch_client:\n                logger.warning(\"OpenSearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n                return self._mock_similarity_search(query_embedding, k, filter_conditions)\n            \n            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä»¥ä¸‹ã‚’ä½¿ç”¨\n            # search_body = {\n            #     \"size\": k,\n            #     \"query\": {\n            #         \"script_score\": {\n            #             \"query\": {\"match_all\": {}},\n            #             \"script\": {\n            #                 \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n            #                 \"params\": {\"query_vector\": query_embedding}\n            #             }\n            #         }\n            #     }\n            # }\n            # \n            # if filter_conditions:\n            #     search_body[\"query\"][\"script_score\"][\"query\"] = {\n            #         \"bool\": {\n            #             \"must\": [{\"match_all\": {}}],\n            #             \"filter\": [filter_conditions]\n            #         }\n            #     }\n            # \n            # response = self.opensearch_client.search(\n            #     index=self.opensearch_index,\n            #     body=search_body\n            # )\n            \n            # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…\n            return self._mock_similarity_search(query_embedding, k, filter_conditions)\n            \n        except Exception as e:\n            logger.error(f\"âŒ é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}\")\n            return {\n                'success': False,\n                'error': str(e),\n                'documents': []\n            }\n    \n    def _mock_similarity_search(self, \n                              query_embedding: List[float], \n                              k: int,\n                              filter_conditions: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"\n        é¡ä¼¼æ¤œç´¢ã®ãƒ¢ãƒƒã‚¯å®Ÿè£…\n        \n        Args:\n            query_embedding: ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿\n            k: å–å¾—ã™ã‚‹æ–‡æ›¸æ•°\n            filter_conditions: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶\n            \n        Returns:\n            Dict: ãƒ¢ãƒƒã‚¯æ¤œç´¢çµæœ\n        \"\"\"\n        logger.info(f\"ğŸ” ãƒ¢ãƒƒã‚¯é¡ä¼¼æ¤œç´¢: k={k}\")\n        \n        # ãƒ€ãƒŸãƒ¼ã®æ¤œç´¢çµæœã‚’ç”Ÿæˆ\n        mock_documents = []\n        for i in range(min(k, 5)):  # æœ€å¤§5ä»¶ã®ãƒ€ãƒŸãƒ¼çµæœ\n            mock_documents.append({\n                'id': f'mock_doc_{i}',\n                'content': f'ã“ã‚Œã¯ãƒ¢ãƒƒã‚¯æ¤œç´¢çµæœ {i+1} ã§ã™ã€‚å®Ÿéš›ã®å®Ÿè£…ã§ã¯é¡ä¼¼åº¦ã®é«˜ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¿”ã•ã‚Œã¾ã™ã€‚',\n                'score': 0.9 - (i * 0.1),  # ã‚¹ã‚³ã‚¢ã‚’é™é †ã§è¨­å®š\n                'metadata': {\n                    'source_file': f'mock_file_{i}.pdf',\n                    'chunk_index': i,\n                    'chunk_type': 'paragraph'\n                }\n            })\n        \n        return {\n            'success': True,\n            'documents': mock_documents,\n            'total_hits': len(mock_documents),\n            'max_score': mock_documents[0]['score'] if mock_documents else 0,\n            'mock': True\n        }\n    \n    def get_embedding_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        åŸ‹ã‚è¾¼ã¿å‡¦ç†çµ±è¨ˆã‚’å–å¾—\n        \n        Returns:\n            Dict: çµ±è¨ˆæƒ…å ±\n        \"\"\"\n        return {\n            'embedding_model': self.embedding_model,\n            'region': self.region,\n            'opensearch_endpoint': self.opensearch_endpoint,\n            'opensearch_index': self.opensearch_index,\n            'embedding_dimension': 1536,  # Titan Embeddings\n            'max_text_length': 8000,\n            'supported_operations': ['generate_embeddings', 'store_to_opensearch', 'similarity_search']\n        }\n\n\ndef create_vector_embedding_processor(config: Dict[str, Any]) -> VectorEmbeddingProcessor:\n    \"\"\"\n    ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿å‡¦ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n    \n    Args:\n        config: è¨­å®šè¾æ›¸\n        \n    Returns:\n        VectorEmbeddingProcessor: å‡¦ç†ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n    \"\"\"\n    return VectorEmbeddingProcessor(\n        region=config.get('region', 'us-east-1'),\n        embedding_model=config.get('embedding_model', 'amazon.titan-embed-text-v1'),\n        opensearch_endpoint=config.get('opensearch_endpoint'),\n        opensearch_index=config.get('opensearch_index', 'documents')\n    )\n\n\n# ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«é–¢æ•°\ndef test_vector_embedding():\n    \"\"\"\n    ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿å‡¦ç†ã®ãƒ†ã‚¹ãƒˆ\n    \"\"\"\n    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ\n    sample_texts = [\n        \"ã“ã‚Œã¯æœ€åˆã®ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚\",\n        \"äºŒç•ªç›®ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯ç•°ãªã‚‹å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\",\n        \"ä¸‰ç•ªç›®ã®ãƒ†ã‚­ã‚¹ãƒˆã¯æŠ€è¡“çš„ãªå†…å®¹ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚\"\n    ]\n    \n    # ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿å‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆ\n    processor = VectorEmbeddingProcessor()\n    \n    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ\n    result = processor.generate_embeddings(sample_texts)\n    print(f\"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆçµæœ: {result.success}\")\n    print(f\"åŸ‹ã‚è¾¼ã¿æ•°: {len(result.embeddings)}\")\n    print(f\"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: {len(result.embeddings[0]) if result.embeddings else 0}\")\n    \n    if result.success:\n        # OpenSearchãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ\n        chunks = [\n            {'content': text, 'metadata': {'chunk_index': i, 'chunk_type': 'paragraph'}}\n            for i, text in enumerate(sample_texts)\n        ]\n        \n        documents = processor.create_opensearch_documents(\n            chunks=chunks,\n            embeddings=result.embeddings,\n            source_file=\"test_document.txt\"\n        )\n        \n        print(f\"OpenSearchãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}\")\n        \n        # OpenSearchã«æ ¼ç´\n        storage_result = processor.store_embeddings_to_opensearch(documents)\n        print(f\"æ ¼ç´çµæœ: {storage_result}\")\n        \n        # é¡ä¼¼æ¤œç´¢ãƒ†ã‚¹ãƒˆ\n        if result.embeddings:\n            search_result = processor.search_similar_documents(\n                query_embedding=result.embeddings[0],\n                k=3\n            )\n            print(f\"æ¤œç´¢çµæœ: {search_result}\")\n\n\nif __name__ == \"__main__\":\n    test_vector_embedding()\n"