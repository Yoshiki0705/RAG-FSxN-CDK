"""
æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›æ©Ÿèƒ½
Markitdownçµ±åˆå‡¦ç†ã®è©³ç´°ãƒ­ã‚°ã¨ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°
"""

import json
import logging
import os
import uuid
from typing import Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)

@dataclass
class LogContext:
    """ãƒ­ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
    correlation_id: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    request_id: Optional[str] = None
    environment: str = 'prod'

class MarkitdownLogger:
    """Markitdownçµ±åˆç”¨æ§‹é€ åŒ–ãƒ­ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, service_name: str = 'document-processor'):
        """
        åˆæœŸåŒ–
        
        Args:
            service_name: ã‚µãƒ¼ãƒ“ã‚¹å
        """
        self.service_name = service_name
        self.environment = os.environ.get('ENVIRONMENT', 'prod')
        self.log_level = os.environ.get('LOG_LEVEL', 'INFO').upper()
        
        # ãƒ­ã‚°è¨­å®š
        self._setup_logger()
        
    def _setup_logger(self):
        """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
        # æ§‹é€ åŒ–ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
        if not logger.handlers:
            handler = logging.StreamHandler()
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(getattr(logging, self.log_level))
    
    def start_document_processing(self, 
                                file_name: str,
                                file_size: int,
                                file_format: str,
                                processing_strategy: str,
                                user_id: Optional[str] = None,
                                project_id: Optional[str] = None) -> str:
        """
        æ–‡æ›¸å‡¦ç†é–‹å§‹ãƒ­ã‚°
        
        Args:
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            file_size: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
            file_format: ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
            processing_strategy: å‡¦ç†æˆ¦ç•¥
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
            
        Returns:
            str: å‡¦ç†ID
        """
        processing_id = str(uuid.uuid4())
        
        log_data = {
            'event_type': 'document_processing_start',
            'processing_id': processing_id,
            'service': self.service_name,
            'environment': self.environment,
            'timestamp': datetime.utcnow().isoformat(),
            'file_info': {
                'name': file_name,
                'size_bytes': file_size,
                'format': file_format
            },
            'processing_config': {
                'strategy': processing_strategy
            },
            'user_context': {
                'user_id': user_id,
                'project_id': project_id
            }
        }
        
        logger.info(f"ğŸš€ æ–‡æ›¸å‡¦ç†é–‹å§‹ | {json.dumps(log_data, ensure_ascii=False)}")
        return processing_id
    
    def log_conversion_attempt(self,
                             method: str,
                             duration_ms: float,
                             success: bool,
                             file_format: str,
                             output_size: int = 0,
                             quality_score: Optional[float] = None,
                             error_message: Optional[str] = None):
        """
        å¤‰æ›è©¦è¡Œãƒ­ã‚°
        
        Args:
            method: å¤‰æ›æ–¹æ³•
            duration_ms: å‡¦ç†æ™‚é–“
            success: æˆåŠŸãƒ•ãƒ©ã‚°
            file_format: ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
            output_size: å‡ºåŠ›ã‚µã‚¤ã‚º
            quality_score: å“è³ªã‚¹ã‚³ã‚¢
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        log_data = {
            'event_type': 'conversion_attempt',
            'service': self.service_name,
            'timestamp': datetime.utcnow().isoformat(),
            'conversion_info': {
                'method': method,
                'file_format': file_format,
                'duration_ms': duration_ms,
                'success': success,
                'output_size_bytes': output_size,
                'quality_score': quality_score
            }
        }
        
        if error_message:
            log_data['error'] = {'message': error_message}
        
        if success:
            logger.info(f"âœ… å¤‰æ›æˆåŠŸ | {json.dumps(log_data, ensure_ascii=False)}")
        else:
            logger.warning(f"âŒ å¤‰æ›å¤±æ•— | {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_langchain_processing(self,
                                chunks_generated: int,
                                duration_ms: float,
                                success: bool,
                                chunk_strategy: str,
                                average_chunk_size: float):
        """
        LangChainå‡¦ç†ãƒ­ã‚°
        
        Args:
            chunks_generated: ç”Ÿæˆãƒãƒ£ãƒ³ã‚¯æ•°
            duration_ms: å‡¦ç†æ™‚é–“
            success: æˆåŠŸãƒ•ãƒ©ã‚°
            chunk_strategy: ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æˆ¦ç•¥
            average_chunk_size: å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        """
        log_data = {
            'event_type': 'langchain_processing',
            'service': self.service_name,
            'timestamp': datetime.utcnow().isoformat(),
            'langchain_info': {
                'chunks_generated': chunks_generated,
                'duration_ms': duration_ms,
                'success': success,
                'chunk_strategy': chunk_strategy,
                'average_chunk_size': average_chunk_size
            }
        }
        
        if success:
            logger.info(f"ğŸ”— LangChainå‡¦ç†å®Œäº† | {json.dumps(log_data, ensure_ascii=False)}")
        else:
            logger.error(f"âŒ LangChainå‡¦ç†å¤±æ•— | {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_embedding_generation(self,
                               model_name: str,
                               embeddings_count: int,
                               duration_ms: float,
                               success: bool,
                               batch_size: int,
                               error_message: Optional[str] = None):
        """
        åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ­ã‚°
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            embeddings_count: åŸ‹ã‚è¾¼ã¿æ•°
            duration_ms: å‡¦ç†æ™‚é–“
            success: æˆåŠŸãƒ•ãƒ©ã‚°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        log_data = {
            'event_type': 'embedding_generation',
            'service': self.service_name,
            'timestamp': datetime.utcnow().isoformat(),
            'embedding_info': {
                'model_name': model_name,
                'embeddings_count': embeddings_count,
                'duration_ms': duration_ms,
                'success': success,
                'batch_size': batch_size
            }
        }
        
        if error_message:
            log_data['error'] = {'message': error_message}
        
        if success:
            logger.info(f"ğŸ”¢ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº† | {json.dumps(log_data, ensure_ascii=False)}")
        else:
            logger.error(f"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•— | {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_storage_operation(self,
                            storage_type: str,
                            documents_stored: int,
                            duration_ms: float,
                            success: bool,
                            index_name: Optional[str] = None):
        """
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ“ä½œãƒ­ã‚°
        
        Args:
            storage_type: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—
            documents_stored: æ ¼ç´ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
            duration_ms: å‡¦ç†æ™‚é–“
            success: æˆåŠŸãƒ•ãƒ©ã‚°
            index_name: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
        """
        log_data = {
            'event_type': 'storage_operation',
            'service': self.service_name,
            'timestamp': datetime.utcnow().isoformat(),
            'storage_info': {
                'storage_type': storage_type,
                'documents_stored': documents_stored,
                'duration_ms': duration_ms,
                'success': success,
                'index_name': index_name
            }
        }
        
        if success:
            logger.info(f"ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ“ä½œå®Œäº† | {json.dumps(log_data, ensure_ascii=False)}")
        else:
            logger.error(f"âŒ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æ“ä½œå¤±æ•— | {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_processing_completion(self,
                                processing_id: str,
                                total_duration_ms: float,
                                success: bool,
                                final_method: str,
                                attempts_count: int,
                                output_size: int = 0):
        """
        å‡¦ç†å®Œäº†ãƒ­ã‚°
        
        Args:
            processing_id: å‡¦ç†ID
            total_duration_ms: ç·å‡¦ç†æ™‚é–“
            success: æˆåŠŸãƒ•ãƒ©ã‚°
            final_method: æœ€çµ‚å‡¦ç†æ–¹æ³•
            attempts_count: è©¦è¡Œå›æ•°
            output_size: å‡ºåŠ›ã‚µã‚¤ã‚º
        """
        log_data = {
            'event_type': 'document_processing_completion',
            'processing_id': processing_id,
            'service': self.service_name,
            'timestamp': datetime.utcnow().isoformat(),
            'completion_info': {
                'total_duration_ms': total_duration_ms,
                'success': success,
                'final_method': final_method,
                'attempts_count': attempts_count,
                'output_size_bytes': output_size
            }
        }
        
        if success:
            logger.info(f"ğŸ‰ æ–‡æ›¸å‡¦ç†å®Œäº† | {json.dumps(log_data, ensure_ascii=False)}")
        else:
            logger.error(f"âŒ æ–‡æ›¸å‡¦ç†å¤±æ•— | {json.dumps(log_data, ensure_ascii=False)}")
    
    def log_error(self,
                 error_type: str,
                 error_message: str,
                 context: Optional[Dict[str, Any]] = None):
        """
        ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
        
        Args:
            error_type: ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
            error_message: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        """
        log_data = {
            'event_type': 'error',
            'service': self.service_name,
            'timestamp': datetime.utcnow().isoformat(),
            'error_info': {
                'type': error_type,
                'message': error_message,
                'context': context or {}
            }
        }
        
        logger.error(f"ğŸ’¥ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ | {json.dumps(log_data, ensure_ascii=False)}")


def create_markitdown_logger(service_name: str = 'document-processor') -> MarkitdownLogger:
    """
    Markitdownçµ±åˆç”¨ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    
    Args:
        service_name: ã‚µãƒ¼ãƒ“ã‚¹å
        
    Returns:
        MarkitdownLogger: ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    return MarkitdownLogger(service_name)


# ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«é–¢æ•°
def test_structured_logging():
    """
    æ§‹é€ åŒ–ãƒ­ã‚°ã®ãƒ†ã‚¹ãƒˆ
    """
    # ãƒ­ã‚°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    markitdown_logger = create_markitdown_logger()
    
    # æ–‡æ›¸å‡¦ç†é–‹å§‹
    processing_id = markitdown_logger.start_document_processing(
        file_name="test_document.pdf",
        file_size=1024000,
        file_format="pdf",
        processing_strategy="both-compare",
        user_id="test_user",
        project_id="test_project"
    )
    
    # å¤‰æ›è©¦è¡Œãƒ­ã‚°
    markitdown_logger.log_conversion_attempt(
        method="markitdown",
        duration_ms=1500.0,
        success=True,
        file_format="pdf",
        output_size=2048000,
        quality_score=85.5
    )
    
    # LangChainå‡¦ç†ãƒ­ã‚°
    markitdown_logger.log_langchain_processing(
        chunks_generated=10,
        duration_ms=800.0,
        success=True,
        chunk_strategy="recursive_character",
        average_chunk_size=1024.0
    )
    
    # å‡¦ç†å®Œäº†ãƒ­ã‚°
    markitdown_logger.log_processing_completion(
        processing_id=processing_id,
        total_duration_ms=3000.0,
        success=True,
        final_method="markitdown",
        attempts_count=1,
        output_size=2048000
    )
    
    print("æ§‹é€ åŒ–ãƒ­ã‚°ãƒ†ã‚¹ãƒˆå®Œäº†")


if __name__ == "__main__":
    test_structured_logging()