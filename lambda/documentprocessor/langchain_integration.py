"""
LangChainçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Markitdownã§å¤‰æ›ã•ã‚ŒãŸãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®LangChainå‡¦ç†çµ±åˆ
"""

import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import hashlib
import re
from datetime import datetime

# LangChain imports (å®Ÿéš›ã®å®Ÿè£…ã§ã¯å¿…è¦)
# from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
# from langchain.schema import Document
# from langchain.embeddings import BedrockEmbeddings
# from langchain.vectorstores import OpenSearchVectorSearch

logger = logging.getLogger(__name__)

@dataclass
class ChunkMetadata:
    """ãƒãƒ£ãƒ³ã‚¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    chunk_id: str
    source_file: str
    chunk_index: int
    chunk_size: int
    chunk_type: str  # 'header', 'paragraph', 'list', 'code', 'table'
    header_level: Optional[int] = None
    parent_header: Optional[str] = None
    processing_method: str = 'markitdown'
    created_at: str = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.utcnow().isoformat()

@dataclass
class ProcessingResult:
    """å‡¦ç†çµæœ"""
    success: bool
    chunks: List[Dict[str, Any]]
    embeddings: List[List[float]]
    metadata: Dict[str, Any]
    error: Optional[str] = None

class LangChainIntegration:
    """LangChainçµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, 
                 region: str = 'us-east-1',
                 embedding_model: str = 'amazon.titan-embed-text-v1',
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200):
        """
        åˆæœŸåŒ–
        
        Args:
            region: AWSãƒªãƒ¼ã‚¸ãƒ§ãƒ³
            embedding_model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            chunk_overlap: ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
        """
        self.region = region
        self.embedding_model = embedding_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä»¥ä¸‹ã‚’åˆæœŸåŒ–
        # self.embeddings = BedrockEmbeddings(
        #     model_id=embedding_model,
        #     region_name=region
        # )
        # self.text_splitter = self._create_text_splitter()
        
        logger.info(f"LangChainçµ±åˆã‚’åˆæœŸåŒ–: region={region}, model={embedding_model}")
    
    def process_markdown_content(self, 
                               markdown_content: str,
                               source_file: str,
                               processing_method: str = 'markitdown',
                               user_id: Optional[str] = None,
                               project_id: Optional[str] = None) -> ProcessingResult:
        """
        ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡¦ç†ã—ã¦ãƒãƒ£ãƒ³ã‚¯ã¨åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
        
        Args:
            markdown_content: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
            processing_method: å‡¦ç†æ–¹æ³•
            user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
            project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
            
        Returns:
            ProcessingResult: å‡¦ç†çµæœ
        """
        try:
            logger.info(f"ğŸ“„ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†é–‹å§‹: {source_file}")
            
            # 1. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
            chunks = self._split_markdown_content(markdown_content, source_file, processing_method)
            
            # 2. å„ãƒãƒ£ãƒ³ã‚¯ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ
            embeddings = self._generate_embeddings([chunk['content'] for chunk in chunks])
            
            # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
            metadata = {
                'source_file': source_file,
                'processing_method': processing_method,
                'total_chunks': len(chunks),
                'total_characters': len(markdown_content),
                'embedding_model': self.embedding_model,
                'chunk_size': self.chunk_size,
                'chunk_overlap': self.chunk_overlap,
                'processed_at': datetime.utcnow().isoformat(),
                'user_id': user_id,
                'project_id': project_id
            }\n            \n            logger.info(f\"âœ… ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†å®Œäº†: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ\")\n            \n            return ProcessingResult(\n                success=True,\n                chunks=chunks,\n                embeddings=embeddings,\n                metadata=metadata\n            )\n            \n        except Exception as e:\n            logger.error(f\"âŒ ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}\")\n            return ProcessingResult(\n                success=False,\n                chunks=[],\n                embeddings=[],\n                metadata={},\n                error=str(e)\n            )\n    \n    def _split_markdown_content(self, \n                              markdown_content: str, \n                              source_file: str,\n                              processing_method: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²\n        \n        Args:\n            markdown_content: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ\n            source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å\n            processing_method: å‡¦ç†æ–¹æ³•\n            \n        Returns:\n            List[Dict]: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ\n        \"\"\"\n        chunks = []\n        \n        # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²ã‚’å®Ÿè¡Œ\n        header_chunks = self._split_by_headers(markdown_content)\n        \n        for i, (content, header_info) in enumerate(header_chunks):\n            # é•·ã„ãƒãƒ£ãƒ³ã‚¯ã‚’ã•ã‚‰ã«åˆ†å‰²\n            if len(content) > self.chunk_size:\n                sub_chunks = self._split_long_chunk(content)\n                for j, sub_content in enumerate(sub_chunks):\n                    chunk_id = self._generate_chunk_id(source_file, i, j)\n                    chunk_metadata = ChunkMetadata(\n                        chunk_id=chunk_id,\n                        source_file=source_file,\n                        chunk_index=len(chunks),\n                        chunk_size=len(sub_content),\n                        chunk_type=self._detect_chunk_type(sub_content),\n                        header_level=header_info.get('level'),\n                        parent_header=header_info.get('title'),\n                        processing_method=processing_method\n                    )\n                    \n                    chunks.append({\n                        'content': sub_content.strip(),\n                        'metadata': chunk_metadata.__dict__\n                    })\n            else:\n                chunk_id = self._generate_chunk_id(source_file, i)\n                chunk_metadata = ChunkMetadata(\n                    chunk_id=chunk_id,\n                    source_file=source_file,\n                    chunk_index=len(chunks),\n                    chunk_size=len(content),\n                    chunk_type=self._detect_chunk_type(content),\n                    header_level=header_info.get('level'),\n                    parent_header=header_info.get('title'),\n                    processing_method=processing_method\n                )\n                \n                chunks.append({\n                    'content': content.strip(),\n                    'metadata': chunk_metadata.__dict__\n                })\n        \n        return chunks\n    \n    def _split_by_headers(self, markdown_content: str) -> List[Tuple[str, Dict[str, Any]]]:\n        \"\"\"\n        ãƒ˜ãƒƒãƒ€ãƒ¼ã«åŸºã¥ã„ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’åˆ†å‰²\n        \n        Args:\n            markdown_content: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ\n            \n        Returns:\n            List[Tuple]: (ã‚³ãƒ³ãƒ†ãƒ³ãƒ„, ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±) ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ\n        \"\"\"\n        # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³\n        header_pattern = r'^(#{1,6})\\s+(.+)$'\n        lines = markdown_content.split('\\n')\n        \n        chunks = []\n        current_chunk = []\n        current_header = {'level': None, 'title': None}\n        \n        for line in lines:\n            header_match = re.match(header_pattern, line)\n            \n            if header_match:\n                # å‰ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜\n                if current_chunk:\n                    chunks.append(('\\n'.join(current_chunk), current_header.copy()))\n                    current_chunk = []\n                \n                # æ–°ã—ã„ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’è¨­å®š\n                level = len(header_match.group(1))\n                title = header_match.group(2).strip()\n                current_header = {'level': level, 'title': title}\n                current_chunk.append(line)\n            else:\n                current_chunk.append(line)\n        \n        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜\n        if current_chunk:\n            chunks.append(('\\n'.join(current_chunk), current_header))\n        \n        return chunks\n    \n    def _split_long_chunk(self, content: str) -> List[str]:\n        \"\"\"\n        é•·ã„ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ†å‰²\n        \n        Args:\n            content: åˆ†å‰²ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n            \n        Returns:\n            List[str]: åˆ†å‰²ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ\n        \"\"\"\n        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ RecursiveCharacterTextSplitter ã‚’ä½¿ç”¨\n        # splitter = RecursiveCharacterTextSplitter(\n        #     chunk_size=self.chunk_size,\n        #     chunk_overlap=self.chunk_overlap,\n        #     separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n        # )\n        # return splitter.split_text(content)\n        \n        # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…\n        chunks = []\n        words = content.split()\n        current_chunk = []\n        current_size = 0\n        \n        for word in words:\n            word_size = len(word) + 1  # ã‚¹ãƒšãƒ¼ã‚¹è¾¼ã¿\n            \n            if current_size + word_size > self.chunk_size and current_chunk:\n                chunks.append(' '.join(current_chunk))\n                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å‡¦ç†\n                overlap_words = current_chunk[-self.chunk_overlap//10:] if len(current_chunk) > self.chunk_overlap//10 else current_chunk\n                current_chunk = overlap_words + [word]\n                current_size = sum(len(w) + 1 for w in current_chunk)\n            else:\n                current_chunk.append(word)\n                current_size += word_size\n        \n        if current_chunk:\n            chunks.append(' '.join(current_chunk))\n        \n        return chunks\n    \n    def _detect_chunk_type(self, content: str) -> str:\n        \"\"\"\n        ãƒãƒ£ãƒ³ã‚¯ã‚¿ã‚¤ãƒ—ã‚’æ¤œå‡º\n        \n        Args:\n            content: ãƒãƒ£ãƒ³ã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n            \n        Returns:\n            str: ãƒãƒ£ãƒ³ã‚¯ã‚¿ã‚¤ãƒ—\n        \"\"\"\n        content_lower = content.lower().strip()\n        \n        # ãƒ˜ãƒƒãƒ€ãƒ¼\n        if re.match(r'^#{1,6}\\s+', content):\n            return 'header'\n        \n        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯\n        if '```' in content or content.startswith('    '):\n            return 'code'\n        \n        # ãƒªã‚¹ãƒˆ\n        if re.match(r'^[\\*\\-\\+]\\s+', content, re.MULTILINE) or re.match(r'^\\d+\\.\\s+', content, re.MULTILINE):\n            return 'list'\n        \n        # ãƒ†ãƒ¼ãƒ–ãƒ«\n        if '|' in content and re.search(r'\\|.*\\|', content):\n            return 'table'\n        \n        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ®µè½\n        return 'paragraph'\n    \n    def _generate_chunk_id(self, source_file: str, chunk_index: int, sub_index: Optional[int] = None) -> str:\n        \"\"\"\n        ãƒãƒ£ãƒ³ã‚¯IDã‚’ç”Ÿæˆ\n        \n        Args:\n            source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å\n            chunk_index: ãƒãƒ£ãƒ³ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n            sub_index: ã‚µãƒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n            \n        Returns:\n            str: ãƒãƒ£ãƒ³ã‚¯ID\n        \"\"\"\n        base_string = f\"{source_file}_{chunk_index}\"\n        if sub_index is not None:\n            base_string += f\"_{sub_index}\"\n        \n        return hashlib.md5(base_string.encode()).hexdigest()[:16]\n    \n    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"\n        ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆ\n        \n        Args:\n            texts: ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ\n            \n        Returns:\n            List[List[float]]: åŸ‹ã‚è¾¼ã¿ãƒªã‚¹ãƒˆ\n        \"\"\"\n        try:\n            logger.info(f\"ğŸ”¢ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆé–‹å§‹: {len(texts)}ãƒ†ã‚­ã‚¹ãƒˆ\")\n            \n            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ Bedrock Embeddings ã‚’ä½¿ç”¨\n            # embeddings = self.embeddings.embed_documents(texts)\n            \n            # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…ï¼ˆå®Ÿéš›ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã¯1536ï¼‰\n            embeddings = []\n            for text in texts:\n                # ãƒ€ãƒŸãƒ¼åŸ‹ã‚è¾¼ã¿ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å‰Šé™¤ï¼‰\n                mock_embedding = [0.1] * 1536  # Titan Embeddings ã®æ¬¡å…ƒæ•°\n                # ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹å¾´ã‚’åæ˜ ã—ãŸãƒ€ãƒŸãƒ¼å€¤\n                text_hash = hash(text) % 1000\n                for i in range(min(10, len(mock_embedding))):\n                    mock_embedding[i] = (text_hash + i) / 1000.0\n                embeddings.append(mock_embedding)\n            \n            logger.info(f\"âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: {len(embeddings)}åŸ‹ã‚è¾¼ã¿\")\n            return embeddings\n            \n        except Exception as e:\n            logger.error(f\"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}\")\n            raise\n    \n    def create_langchain_documents(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"\n        LangChain Document ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n        \n        Args:\n            chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ\n            \n        Returns:\n            List[Dict]: LangChain Document äº’æ›ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n        \"\"\"\n        documents = []\n        \n        for chunk in chunks:\n            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ langchain.schema.Document ã‚’ä½¿ç”¨\n            # doc = Document(\n            #     page_content=chunk['content'],\n            #     metadata=chunk['metadata']\n            # )\n            \n            # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…\n            doc = {\n                'page_content': chunk['content'],\n                'metadata': chunk['metadata'],\n                'type': 'Document'\n            }\n            documents.append(doc)\n        \n        return documents\n    \n    def get_processing_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        å‡¦ç†çµ±è¨ˆã‚’å–å¾—\n        \n        Returns:\n            Dict: å‡¦ç†çµ±è¨ˆ\n        \"\"\"\n        return {\n            'embedding_model': self.embedding_model,\n            'chunk_size': self.chunk_size,\n            'chunk_overlap': self.chunk_overlap,\n            'region': self.region,\n            'supported_chunk_types': ['header', 'paragraph', 'list', 'code', 'table']\n        }\n\n\ndef create_langchain_integration(config: Dict[str, Any]) -> LangChainIntegration:\n    \"\"\"\n    LangChainçµ±åˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n    \n    Args:\n        config: è¨­å®šè¾æ›¸\n        \n    Returns:\n        LangChainIntegration: çµ±åˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n    \"\"\"\n    return LangChainIntegration(\n        region=config.get('region', 'us-east-1'),\n        embedding_model=config.get('embedding_model', 'amazon.titan-embed-text-v1'),\n        chunk_size=config.get('chunk_size', 1000),\n        chunk_overlap=config.get('chunk_overlap', 200)\n    )\n\n\n# ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«é–¢æ•°\ndef test_langchain_integration():\n    \"\"\"\n    LangChainçµ±åˆã®ãƒ†ã‚¹ãƒˆ\n    \"\"\"\n    # ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n    sample_markdown = \"\"\"\n# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒˆãƒ«\n\nã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚\n\n## ã‚»ã‚¯ã‚·ãƒ§ãƒ³1\n\nã‚»ã‚¯ã‚·ãƒ§ãƒ³1ã®å†…å®¹ã§ã™ã€‚\n\n### ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³1.1\n\nã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å†…å®¹ã§ã™ã€‚\n\n- ãƒªã‚¹ãƒˆé …ç›®1\n- ãƒªã‚¹ãƒˆé …ç›®2\n- ãƒªã‚¹ãƒˆé …ç›®3\n\n## ã‚»ã‚¯ã‚·ãƒ§ãƒ³2\n\n```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\n| åˆ—1 | åˆ—2 | åˆ—3 |\n|-----|-----|-----|\n| A   | B   | C   |\n| D   | E   | F   |\n\"\"\"\n    \n    # LangChainçµ±åˆã‚’ãƒ†ã‚¹ãƒˆ\n    integration = LangChainIntegration()\n    result = integration.process_markdown_content(\n        markdown_content=sample_markdown,\n        source_file=\"test_document.md\",\n        processing_method=\"markitdown\"\n    )\n    \n    print(f\"å‡¦ç†çµæœ: {result.success}\")\n    print(f\"ãƒãƒ£ãƒ³ã‚¯æ•°: {len(result.chunks)}\")\n    print(f\"åŸ‹ã‚è¾¼ã¿æ•°: {len(result.embeddings)}\")\n    \n    for i, chunk in enumerate(result.chunks[:3]):  # æœ€åˆã®3ãƒãƒ£ãƒ³ã‚¯ã‚’è¡¨ç¤º\n        print(f\"\\nãƒãƒ£ãƒ³ã‚¯ {i+1}:\")\n        print(f\"ã‚¿ã‚¤ãƒ—: {chunk['metadata']['chunk_type']}\")\n        print(f\"ã‚µã‚¤ã‚º: {chunk['metadata']['chunk_size']}\")\n        print(f\"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {chunk['content'][:100]}...\")\n\n\nif __name__ == \"__main__\":\n    test_langchain_integration()\n"