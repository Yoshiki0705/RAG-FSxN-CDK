# ğŸ’° Cost Optimization Guide

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Embedding Batch Workload Template ã®ã‚³ã‚¹ãƒˆæœ€é©åŒ–æˆ¦ç•¥ã€ãƒªã‚½ãƒ¼ã‚¹ã‚µã‚¤ã‚¸ãƒ³ã‚°ã€äºˆç®—ç®¡ç†ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

- [ğŸ’¡ ã‚³ã‚¹ãƒˆæœ€é©åŒ–æˆ¦ç•¥](#-ã‚³ã‚¹ãƒˆæœ€é©åŒ–æˆ¦ç•¥)
- [ğŸ“Š ã‚³ã‚¹ãƒˆåˆ†æ](#-ã‚³ã‚¹ãƒˆåˆ†æ)
- [âš™ï¸ ãƒªã‚½ãƒ¼ã‚¹ã‚µã‚¤ã‚¸ãƒ³ã‚°](#ï¸-ãƒªã‚½ãƒ¼ã‚¹ã‚µã‚¤ã‚¸ãƒ³ã‚°)
- [ğŸ¯ Spot ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ´»ç”¨](#-spot-ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ´»ç”¨)
- [ğŸ“ˆ äºˆç®—ç®¡ç†](#-äºˆç®—ç®¡ç†)
- [ğŸ”„ è‡ªå‹•åŒ–](#-è‡ªå‹•åŒ–)

---

## ğŸ’¡ ã‚³ã‚¹ãƒˆæœ€é©åŒ–æˆ¦ç•¥

### ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®5ã¤ã®æŸ±

#### 1. Right Sizing (é©åˆ‡ãªã‚µã‚¤ã‚¸ãƒ³ã‚°)
- **ç›®æ¨™**: ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«æœ€é©ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¨ã‚µã‚¤ã‚ºã®é¸æŠ
- **æ‰‹æ³•**: ä½¿ç”¨ç‡ç›£è¦–ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã€æ®µéšçš„ã‚µã‚¤ã‚¸ãƒ³ã‚°

#### 2. Elasticity (å¼¾åŠ›æ€§)
- **ç›®æ¨™**: éœ€è¦ã«å¿œã˜ãŸè‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
- **æ‰‹æ³•**: Auto Scalingã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°

#### 3. Optimal Pricing Model (æœ€é©ãªæ–™é‡‘ãƒ¢ãƒ‡ãƒ«)
- **ç›®æ¨™**: Spotã€Reservedã€Savings Plans ã®æ´»ç”¨
- **æ‰‹æ³•**: ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã€ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒ³ãƒˆæˆ¦ç•¥

#### 4. Optimize Data Transfer (ãƒ‡ãƒ¼ã‚¿è»¢é€æœ€é©åŒ–)
- **ç›®æ¨™**: ä¸è¦ãªãƒ‡ãƒ¼ã‚¿è»¢é€ã®å‰Šæ¸›
- **æ‰‹æ³•**: VPC Endpointsã€ãƒªãƒ¼ã‚¸ãƒ§ãƒ³æœ€é©åŒ–

#### 5. Monitor & Analyze (ç›£è¦–ãƒ»åˆ†æ)
- **ç›®æ¨™**: ç¶™ç¶šçš„ãªã‚³ã‚¹ãƒˆç›£è¦–ã¨æœ€é©åŒ–
- **æ‰‹æ³•**: Cost Explorerã€Budgetsã€Trusted Advisor

### ã‚³ã‚¹ãƒˆæ§‹é€ ã®ç†è§£

```mermaid
pie title Cost Breakdown (Typical)
    "EC2 Compute" : 45
    "FSx Storage" : 25
    "Bedrock API" : 15
    "Data Transfer" : 8
    "Other Services" : 7
```

---

## ğŸ“Š ã‚³ã‚¹ãƒˆåˆ†æ

### 1. ç¾åœ¨ã®ã‚³ã‚¹ãƒˆåˆ†æ

#### Cost Explorer ã«ã‚ˆã‚‹åˆ†æ
```bash
#!/bin/bash
# cost-analysis.sh

PROJECT_NAME="embedding-batch-workload"
ENVIRONMENT="production"
START_DATE=$(date -d '30 days ago' +%Y-%m-%d)
END_DATE=$(date +%Y-%m-%d)

echo "=== Cost Analysis ($START_DATE to $END_DATE) ==="

# ã‚µãƒ¼ãƒ“ã‚¹åˆ¥ã‚³ã‚¹ãƒˆ
aws ce get-cost-and-usage \
  --time-period Start=$START_DATE,End=$END_DATE \
  --granularity MONTHLY \
  --metrics BlendedCost \
  --group-by Type=DIMENSION,Key=SERVICE \
  --filter file://cost-filter.json \
  --query 'ResultsByTime[0].Groups[*].{Service:Keys[0],Cost:Metrics.BlendedCost.Amount}' \
  --output table

# ãƒªã‚½ãƒ¼ã‚¹åˆ¥ã‚³ã‚¹ãƒˆ
aws ce get-cost-and-usage \
  --time-period Start=$START_DATE,End=$END_DATE \
  --granularity MONTHLY \
  --metrics BlendedCost \
  --group-by Type=DIMENSION,Key=RESOURCE_ID \
  --filter file://cost-filter.json \
  --query 'ResultsByTime[0].Groups[*].{Resource:Keys[0],Cost:Metrics.BlendedCost.Amount}' \
  --output table
```

#### ã‚³ã‚¹ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š
```json
{
  "Dimensions": {
    "Key": "RESOURCE_ID",
    "Values": [
      "*embedding-batch-workload*"
    ],
    "MatchOptions": ["CONTAINS"]
  }
}
```

### 2. ã‚³ã‚¹ãƒˆå‚¾å‘åˆ†æ

#### Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚‹è©³ç´°åˆ†æ
```python
#!/usr/bin/env python3
# cost-trend-analysis.py

import boto3
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import json

class CostAnalyzer:
    def __init__(self, project_name, environment):
        self.project_name = project_name
        self.environment = environment
        self.ce_client = boto3.client('ce')
        self.cloudwatch = boto3.client('cloudwatch')
    
    def get_cost_trends(self, days=30):
        """éå»Næ—¥é–“ã®ã‚³ã‚¹ãƒˆå‚¾å‘ã‚’å–å¾—"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        response = self.ce_client.get_cost_and_usage(
            TimePeriod={
                'Start': start_date.strftime('%Y-%m-%d'),
                'End': end_date.strftime('%Y-%m-%d')
            },
            Granularity='DAILY',
            Metrics=['BlendedCost'],
            GroupBy=[
                {
                    'Type': 'DIMENSION',
                    'Key': 'SERVICE'
                }
            ],
            Filter={
                'Dimensions': {
                    'Key': 'RESOURCE_ID',
                    'Values': [f'*{self.project_name}*'],
                    'MatchOptions': ['CONTAINS']
                }
            }
        )
        
        return response
    
    def analyze_usage_patterns(self):
        """ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        # Batch ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
        metrics = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/Batch',
            MetricName='RunningJobs',
            Dimensions=[
                {
                    'Name': 'JobQueue',
                    'Value': f'{self.project_name}-{self.environment}-job-queue'
                }
            ],
            StartTime=datetime.now() - timedelta(days=7),
            EndTime=datetime.now(),
            Period=3600,
            Statistics=['Average', 'Maximum']
        )
        
        return metrics
    
    def generate_recommendations(self):
        """ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # ä½¿ç”¨ç‡ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨
        usage_data = self.analyze_usage_patterns()
        avg_usage = sum(d['Average'] for d in usage_data['Datapoints']) / len(usage_data['Datapoints'])
        
        if avg_usage < 0.3:  # 30%æœªæº€ã®ä½¿ç”¨ç‡
            recommendations.append({
                'category': 'Right Sizing',
                'recommendation': 'Consider reducing instance types or using smaller instances',
                'potential_savings': '20-40%',
                'priority': 'High'
            })
        
        # Spot ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®æ¨å¥¨
        recommendations.append({
            'category': 'Pricing Model',
            'recommendation': 'Implement Spot instances for fault-tolerant workloads',
            'potential_savings': '50-70%',
            'priority': 'High'
        })
        
        return recommendations
    
    def generate_report(self):
        """åŒ…æ‹¬çš„ãªã‚³ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        cost_trends = self.get_cost_trends()
        usage_patterns = self.analyze_usage_patterns()
        recommendations = self.generate_recommendations()
        
        report = {
            'analysis_date': datetime.now().isoformat(),
            'project': f'{self.project_name}-{self.environment}',
            'cost_trends': cost_trends,
            'usage_patterns': usage_patterns,
            'recommendations': recommendations
        }
        
        return report

if __name__ == "__main__":
    analyzer = CostAnalyzer("embedding-batch-workload", "production")
    report = analyzer.generate_report()
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONå½¢å¼ã§å‡ºåŠ›
    print(json.dumps(report, indent=2, default=str))
```

---

## âš™ï¸ ãƒªã‚½ãƒ¼ã‚¹ã‚µã‚¤ã‚¸ãƒ³ã‚°

### 1. Compute Environment ã‚µã‚¤ã‚¸ãƒ³ã‚°

#### ä½¿ç”¨ç‡ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚¸ãƒ³ã‚°
```yaml
# ä½ä½¿ç”¨ç‡ç’°å¢ƒï¼ˆé–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆï¼‰
DevelopmentComputeEnvironment:
  Type: AWS::Batch::ComputeEnvironment
  Properties:
    ComputeResources:
      Type: EC2
      MinvCpus: 0
      MaxvCpus: 50
      DesiredvCpus: 0
      InstanceTypes:
        - t3.medium
        - t3.large
      AllocationStrategy: BEST_FIT_PROGRESSIVE

# ä¸­ä½¿ç”¨ç‡ç’°å¢ƒï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ï¼‰
StagingComputeEnvironment:
  Type: AWS::Batch::ComputeEnvironment
  Properties:
    ComputeResources:
      Type: EC2
      MinvCpus: 0
      MaxvCpus: 200
      DesiredvCpus: 5
      InstanceTypes:
        - m5.large
        - m5.xlarge
      AllocationStrategy: BEST_FIT_PROGRESSIVE

# é«˜ä½¿ç”¨ç‡ç’°å¢ƒï¼ˆæœ¬ç•ªï¼‰
ProductionComputeEnvironment:
  Type: AWS::Batch::ComputeEnvironment
  Properties:
    ComputeResources:
      Type: EC2
      MinvCpus: 10
      MaxvCpus: 1000
      DesiredvCpus: 50
      InstanceTypes:
        - m5.large
        - m5.xlarge
        - m5.2xlarge
        - c5.large
        - c5.xlarge
      AllocationStrategy: DIVERSIFIED
```

### 2. ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—é¸æŠ

#### ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰åˆ¥æœ€é©åŒ–
```bash
#!/bin/bash
# instance-type-optimizer.sh

echo "=== Instance Type Optimization Analysis ==="

# CPUé›†ç´„çš„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰
echo "CPU-Intensive Workloads:"
echo "Recommended: c5.large, c5.xlarge, c5.2xlarge"
echo "Cost per vCPU hour: $0.017 - $0.068"

# ãƒ¡ãƒ¢ãƒªé›†ç´„çš„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰
echo "Memory-Intensive Workloads:"
echo "Recommended: r5.large, r5.xlarge, r5.2xlarge"
echo "Cost per GB RAM hour: $0.013 - $0.052"

# ãƒãƒ©ãƒ³ã‚¹å‹ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰
echo "Balanced Workloads:"
echo "Recommended: m5.large, m5.xlarge, m5.2xlarge"
echo "Cost per hour: $0.096 - $0.384"

# GPU ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰
echo "GPU Workloads:"
echo "Recommended: g4dn.xlarge, g4dn.2xlarge"
echo "Cost per hour: $0.526 - $0.752"
```

#### å‹•çš„ã‚µã‚¤ã‚¸ãƒ³ã‚°
```python
#!/usr/bin/env python3
# dynamic-sizing.py

import boto3
import json
from datetime import datetime, timedelta

class DynamicSizer:
    def __init__(self, compute_env_name):
        self.compute_env_name = compute_env_name
        self.batch_client = boto3.client('batch')
        self.cloudwatch = boto3.client('cloudwatch')
    
    def analyze_current_usage(self):
        """ç¾åœ¨ã®ä½¿ç”¨çŠ¶æ³ã‚’åˆ†æ"""
        # éå»7æ—¥é–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        end_time = datetime.now()
        start_time = end_time - timedelta(days=7)
        
        metrics = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/Batch',
            MetricName='RunningJobs',
            Dimensions=[
                {
                    'Name': 'ComputeEnvironment',
                    'Value': self.compute_env_name
                }
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=3600,
            Statistics=['Average', 'Maximum']
        )
        
        if not metrics['Datapoints']:
            return None
        
        avg_jobs = sum(d['Average'] for d in metrics['Datapoints']) / len(metrics['Datapoints'])
        max_jobs = max(d['Maximum'] for d in metrics['Datapoints'])
        
        return {
            'average_running_jobs': avg_jobs,
            'maximum_running_jobs': max_jobs,
            'utilization_pattern': self.classify_pattern(avg_jobs, max_jobs)
        }
    
    def classify_pattern(self, avg_jobs, max_jobs):
        """ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†é¡"""
        if max_jobs / avg_jobs > 3:
            return 'bursty'  # ãƒãƒ¼ã‚¹ãƒˆå‹
        elif avg_jobs > 10:
            return 'steady'  # å®‰å®šå‹
        else:
            return 'low'     # ä½ä½¿ç”¨ç‡
    
    def recommend_sizing(self):
        """ã‚µã‚¤ã‚¸ãƒ³ã‚°æ¨å¥¨ã‚’ç”Ÿæˆ"""
        usage = self.analyze_current_usage()
        
        if not usage:
            return {'error': 'Insufficient data for analysis'}
        
        pattern = usage['utilization_pattern']
        max_jobs = usage['maximum_running_jobs']
        
        if pattern == 'bursty':
            return {
                'min_vcpus': 0,
                'max_vcpus': int(max_jobs * 4),  # ãƒãƒ¼ã‚¹ãƒˆã«å¯¾å¿œ
                'desired_vcpus': int(usage['average_running_jobs'] * 2),
                'instance_types': ['m5.large', 'm5.xlarge', 'm5.2xlarge'],
                'allocation_strategy': 'DIVERSIFIED'
            }
        elif pattern == 'steady':
            return {
                'min_vcpus': int(usage['average_running_jobs']),
                'max_vcpus': int(max_jobs * 2),
                'desired_vcpus': int(usage['average_running_jobs'] * 1.2),
                'instance_types': ['m5.xlarge', 'm5.2xlarge'],
                'allocation_strategy': 'BEST_FIT_PROGRESSIVE'
            }
        else:  # low usage
            return {
                'min_vcpus': 0,
                'max_vcpus': int(max_jobs * 2),
                'desired_vcpus': 0,
                'instance_types': ['t3.medium', 't3.large'],
                'allocation_strategy': 'BEST_FIT'
            }

if __name__ == "__main__":
    sizer = DynamicSizer("embedding-batch-workload-prod-compute-env")
    recommendation = sizer.recommend_sizing()
    print(json.dumps(recommendation, indent=2))
```

---

## ğŸ¯ Spot ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ´»ç”¨

### 1. Spot ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹è¨­å®š

#### CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
```yaml
SpotComputeEnvironment:
  Type: AWS::Batch::ComputeEnvironment
  Properties:
    Type: MANAGED
    State: ENABLED
    ServiceRole: !GetAtt BatchServiceRole.Arn
    ComputeResources:
      Type: EC2
      MinvCpus: 0
      MaxvCpus: !Ref MaxvCpus
      DesiredvCpus: 0
      InstanceTypes:
        - m5.large
        - m5.xlarge
        - m5.2xlarge
        - c5.large
        - c5.xlarge
      AllocationStrategy: SPOT_CAPACITY_OPTIMIZED
      BidPercentage: 50  # On-Demandä¾¡æ ¼ã®50%ã¾ã§
      Subnets: !Split [',', !Ref SubnetIds]
      SecurityGroupIds:
        - !Ref BatchSecurityGroup
      InstanceRole: !GetAtt BatchInstanceProfile.Arn
      SpotIamFleetRequestRole: !GetAtt SpotFleetRole.Arn
      Tags:
        Project: !Ref ProjectName
        Environment: !Ref Environment
        CostOptimization: "Spot"

# Spot Fleetç”¨IAMãƒ­ãƒ¼ãƒ«
SpotFleetRole:
  Type: AWS::IAM::Role
  Properties:
    AssumeRolePolicyDocument:
      Version: '2012-10-17'
      Statement:
        - Effect: Allow
          Principal:
            Service: spotfleet.amazonaws.com
          Action: sts:AssumeRole
    ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole
```

### 2. Spot ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç›£è¦–

#### Spot ä¸­æ–­ç›£è¦–
```bash
#!/bin/bash
# spot-interruption-monitor.sh

PROJECT_NAME="embedding-batch-workload"
ENVIRONMENT="production"

echo "=== Spot Instance Interruption Monitoring ==="

# Spotä¸­æ–­ã®å±¥æ­´ç¢ºèª
aws ec2 describe-spot-price-history \
  --instance-types m5.large m5.xlarge c5.large c5.xlarge \
  --product-descriptions "Linux/UNIX" \
  --start-time $(date -d '24 hours ago' --iso-8601) \
  --end-time $(date --iso-8601) \
  --query 'SpotPriceHistory[*].{InstanceType:InstanceType,SpotPrice:SpotPrice,Timestamp:Timestamp}' \
  --output table

# ç¾åœ¨ã®Spotä¾¡æ ¼
echo "Current Spot Prices:"
aws ec2 describe-spot-price-history \
  --instance-types m5.large m5.xlarge c5.large c5.xlarge \
  --product-descriptions "Linux/UNIX" \
  --max-items 10 \
  --query 'SpotPriceHistory[*].{InstanceType:InstanceType,SpotPrice:SpotPrice,AvailabilityZone:AvailabilityZone}' \
  --output table

# Spotä¸­æ–­ã«ã‚ˆã‚‹ã‚¸ãƒ§ãƒ–å¤±æ•—ã®ç¢ºèª
echo "Jobs Failed Due to Spot Interruption:"
aws batch list-jobs \
  --job-queue "${PROJECT_NAME}-${ENVIRONMENT}-job-queue" \
  --job-status FAILED \
  --query 'jobSummary[?contains(statusReason, `Spot`)].{JobId:jobId,JobName:jobName,StatusReason:statusReason}' \
  --output table
```

### 3. æ··åˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æˆ¦ç•¥

#### On-Demand + Spot æ··åˆè¨­å®š
```yaml
MixedComputeEnvironment:
  Type: AWS::Batch::ComputeEnvironment
  Properties:
    Type: MANAGED
    State: ENABLED
    ServiceRole: !GetAtt BatchServiceRole.Arn
    ComputeResources:
      Type: EC2
      MinvCpus: 10  # æœ€å°å®¹é‡ã¯On-Demandã§ç¢ºä¿
      MaxvCpus: !Ref MaxvCpus
      DesiredvCpus: 20
      InstanceTypes:
        - m5.large
        - m5.xlarge
        - c5.large
        - c5.xlarge
      AllocationStrategy: BEST_FIT_PROGRESSIVE
      # 70% Spot, 30% On-Demand ã®æ··åˆ
      BidPercentage: 60
      Subnets: !Split [',', !Ref SubnetIds]
      SecurityGroupIds:
        - !Ref BatchSecurityGroup
      InstanceRole: !GetAtt BatchInstanceProfile.Arn

# é‡è¦ãªã‚¸ãƒ§ãƒ–ç”¨ã®On-Demandã‚­ãƒ¥ãƒ¼
CriticalJobQueue:
  Type: AWS::Batch::JobQueue
  Properties:
    JobQueueName: !Sub "${ProjectName}-${Environment}-critical-queue"
    State: ENABLED
    Priority: 100
    ComputeEnvironmentOrder:
      - Order: 1
        ComputeEnvironment: !Ref OnDemandComputeEnvironment

# é€šå¸¸ã‚¸ãƒ§ãƒ–ç”¨ã®Spotã‚­ãƒ¥ãƒ¼
RegularJobQueue:
  Type: AWS::Batch::JobQueue
  Properties:
    JobQueueName: !Sub "${ProjectName}-${Environment}-regular-queue"
    State: ENABLED
    Priority: 50
    ComputeEnvironmentOrder:
      - Order: 1
        ComputeEnvironment: !Ref SpotComputeEnvironment
      - Order: 2
        ComputeEnvironment: !Ref OnDemandComputeEnvironment
```

---

## ğŸ“ˆ äºˆç®—ç®¡ç†

### 1. AWS Budgets è¨­å®š

#### æœˆæ¬¡äºˆç®—ã‚¢ãƒ©ãƒ¼ãƒˆ
```yaml
MonthlyBudget:
  Type: AWS::Budgets::Budget
  Properties:
    Budget:
      BudgetName: !Sub "${ProjectName}-${Environment}-monthly-budget"
      BudgetLimit:
        Amount: !Ref MonthlyBudgetAmount
        Unit: USD
      TimeUnit: MONTHLY
      BudgetType: COST
      CostFilters:
        TagKey:
          - Project
        TagValue:
          - !Ref ProjectName
    NotificationsWithSubscribers:
      - Notification:
          NotificationType: ACTUAL
          ComparisonOperator: GREATER_THAN
          Threshold: 80
          ThresholdType: PERCENTAGE
        Subscribers:
          - SubscriptionType: EMAIL
            Address: !Ref BudgetAlertEmail
      - Notification:
          NotificationType: FORECASTED
          ComparisonOperator: GREATER_THAN
          Threshold: 100
          ThresholdType: PERCENTAGE
        Subscribers:
          - SubscriptionType: EMAIL
            Address: !Ref BudgetAlertEmail

# æ—¥æ¬¡äºˆç®—ç›£è¦–
DailyBudget:
  Type: AWS::Budgets::Budget
  Properties:
    Budget:
      BudgetName: !Sub "${ProjectName}-${Environment}-daily-budget"
      BudgetLimit:
        Amount: !Ref DailyBudgetAmount
        Unit: USD
      TimeUnit: DAILY
      BudgetType: COST
      CostFilters:
        TagKey:
          - Project
        TagValue:
          - !Ref ProjectName
    NotificationsWithSubscribers:
      - Notification:
          NotificationType: ACTUAL
          ComparisonOperator: GREATER_THAN
          Threshold: 100
          ThresholdType: PERCENTAGE
        Subscribers:
          - SubscriptionType: EMAIL
            Address: !Ref BudgetAlertEmail
```

### 2. ã‚³ã‚¹ãƒˆç•°å¸¸æ¤œçŸ¥

#### Cost Anomaly Detection
```yaml
CostAnomalyDetector:
  Type: AWS::CE::AnomalyDetector
  Properties:
    AnomalyDetectorName: !Sub "${ProjectName}-${Environment}-anomaly-detector"
    MonitorType: DIMENSIONAL
    MonitorSpecification: |
      {
        "Dimension": "SERVICE",
        "Key": "SERVICE",
        "Values": ["Amazon Elastic Compute Cloud - Compute", "Amazon FSx", "Amazon Bedrock"],
        "MatchOptions": ["EQUALS"]
      }

CostAnomalySubscription:
  Type: AWS::CE::AnomalySubscription
  Properties:
    SubscriptionName: !Sub "${ProjectName}-${Environment}-anomaly-subscription"
    MonitorArnList:
      - !GetAtt CostAnomalyDetector.AnomalyDetectorArn
    Subscribers:
      - Type: EMAIL
        Address: !Ref CostAnomalyEmail
    Threshold: 100  # $100ä»¥ä¸Šã®ç•°å¸¸ã‚’æ¤œçŸ¥
    Frequency: DAILY
```

### 3. è‡ªå‹•ã‚³ã‚¹ãƒˆåˆ¶å¾¡

#### Lambda ã«ã‚ˆã‚‹è‡ªå‹•åœæ­¢
```python
#!/usr/bin/env python3
# auto-cost-control.py

import boto3
import json
import os
from datetime import datetime

def lambda_handler(event, context):
    """äºˆç®—è¶…éæ™‚ã®è‡ªå‹•ã‚³ã‚¹ãƒˆåˆ¶å¾¡"""
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
    project_name = os.environ['PROJECT_NAME']
    environment = os.environ['ENVIRONMENT']
    max_daily_cost = float(os.environ['MAX_DAILY_COST'])
    
    # AWSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    ce_client = boto3.client('ce')
    batch_client = boto3.client('batch')
    sns_client = boto3.client('sns')
    
    # ä»Šæ—¥ã®ã‚³ã‚¹ãƒˆã‚’å–å¾—
    today = datetime.now().strftime('%Y-%m-%d')
    
    response = ce_client.get_cost_and_usage(
        TimePeriod={
            'Start': today,
            'End': today
        },
        Granularity='DAILY',
        Metrics=['BlendedCost'],
        Filter={
            'Dimensions': {
                'Key': 'RESOURCE_ID',
                'Values': [f'*{project_name}*'],
                'MatchOptions': ['CONTAINS']
            }
        }
    )
    
    if not response['ResultsByTime']:
        return {'statusCode': 200, 'body': 'No cost data available'}
    
    daily_cost = float(response['ResultsByTime'][0]['Total']['BlendedCost']['Amount'])
    
    # äºˆç®—è¶…éãƒã‚§ãƒƒã‚¯
    if daily_cost > max_daily_cost:
        # Compute Environment ã‚’ç„¡åŠ¹åŒ–
        compute_env_name = f"{project_name}-{environment}-compute-env"
        
        try:
            batch_client.update_compute_environment(
                computeEnvironment=compute_env_name,
                state='DISABLED'
            )
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
            message = f"""
            ğŸš¨ COST ALERT: Daily budget exceeded!
            
            Project: {project_name}-{environment}
            Daily Cost: ${daily_cost:.2f}
            Budget Limit: ${max_daily_cost:.2f}
            
            Action Taken: Compute Environment disabled
            """
            
            sns_client.publish(
                TopicArn=os.environ['ALERT_TOPIC_ARN'],
                Subject=f"Cost Alert: {project_name}-{environment}",
                Message=message
            )
            
            return {
                'statusCode': 200,
                'body': f'Cost control activated. Daily cost: ${daily_cost:.2f}'
            }
            
        except Exception as e:
            return {
                'statusCode': 500,
                'body': f'Error in cost control: {str(e)}'
            }
    
    return {
        'statusCode': 200,
        'body': f'Daily cost within budget: ${daily_cost:.2f}'
    }
```

---

## ğŸ”„ è‡ªå‹•åŒ–

### 1. ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®è‡ªå‹•åŒ–

#### é€±æ¬¡æœ€é©åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash
# weekly-cost-optimization.sh

PROJECT_NAME="embedding-batch-workload"
ENVIRONMENT="production"
REPORT_DATE=$(date +%Y-%m-%d)

echo "=== Weekly Cost Optimization - $REPORT_DATE ==="

# 1. ä½¿ç”¨ç‡åˆ†æ
echo "1. Analyzing resource utilization..."
python3 analyze-utilization.py --project "$PROJECT_NAME" --env "$ENVIRONMENT"

# 2. Spotä¾¡æ ¼åˆ†æ
echo "2. Analyzing Spot pricing opportunities..."
aws ec2 describe-spot-price-history \
  --instance-types m5.large m5.xlarge c5.large c5.xlarge \
  --product-descriptions "Linux/UNIX" \
  --start-time $(date -d '7 days ago' --iso-8601) \
  --end-time $(date --iso-8601) \
  --query 'SpotPriceHistory | sort_by(@, &Timestamp) | [-1].{InstanceType:InstanceType,SpotPrice:SpotPrice}' \
  --output table

# 3. æœªä½¿ç”¨ãƒªã‚½ãƒ¼ã‚¹ã®ç‰¹å®š
echo "3. Identifying unused resources..."
# åœæ­¢ä¸­ã®EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
aws ec2 describe-instances \
  --filters "Name=tag:Project,Values=$PROJECT_NAME" "Name=instance-state-name,Values=stopped" \
  --query 'Reservations[*].Instances[*].{InstanceId:InstanceId,InstanceType:InstanceType,LaunchTime:LaunchTime}' \
  --output table

# 4. ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–
echo "4. Storage optimization analysis..."
# å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
aws ec2 describe-snapshots \
  --owner-ids self \
  --filters "Name=tag:Project,Values=$PROJECT_NAME" \
  --query 'Snapshots[?StartTime<=`'$(date -d '30 days ago' --iso-8601)'`].{SnapshotId:SnapshotId,StartTime:StartTime,VolumeSize:VolumeSize}' \
  --output table

# 5. æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
echo "5. Generating optimization recommendations..."
python3 generate-cost-recommendations.py --project "$PROJECT_NAME" --env "$ENVIRONMENT"

echo "=== Weekly Cost Optimization Complete ==="
```

### 2. è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æœ€é©åŒ–

#### CloudWatch Events ã«ã‚ˆã‚‹è‡ªå‹•èª¿æ•´
```yaml
ScheduledScalingRule:
  Type: AWS::Events::Rule
  Properties:
    Description: "Scale down compute environment during off-hours"
    ScheduleExpression: "cron(0 18 * * MON-FRI)"  # å¹³æ—¥18æ™‚
    State: ENABLED
    Targets:
      - Arn: !GetAtt ScaleDownFunction.Arn
        Id: "ScaleDownTarget"

ScaleUpRule:
  Type: AWS::Events::Rule
  Properties:
    Description: "Scale up compute environment during business hours"
    ScheduleExpression: "cron(0 8 * * MON-FRI)"   # å¹³æ—¥8æ™‚
    State: ENABLED
    Targets:
      - Arn: !GetAtt ScaleUpFunction.Arn
        Id: "ScaleUpTarget"

ScaleDownFunction:
  Type: AWS::Lambda::Function
  Properties:
    FunctionName: !Sub "${ProjectName}-${Environment}-scale-down"
    Runtime: python3.9
    Handler: index.lambda_handler
    Code:
      ZipFile: |
        import boto3
        import os
        
        def lambda_handler(event, context):
            batch_client = boto3.client('batch')
            compute_env = os.environ['COMPUTE_ENVIRONMENT_NAME']
            
            # Desired capacity ã‚’0ã«è¨­å®š
            response = batch_client.update_compute_environment(
                computeEnvironment=compute_env,
                computeResources={
                    'desiredvCpus': 0
                }
            )
            
            return {
                'statusCode': 200,
                'body': f'Scaled down {compute_env}'
            }
    Environment:
      Variables:
        COMPUTE_ENVIRONMENT_NAME: !Ref BatchComputeEnvironment
```

### 3. ã‚³ã‚¹ãƒˆç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

#### è‡ªå‹•ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°
```python
#!/usr/bin/env python3
# update-cost-dashboard.py

import boto3
import json
from datetime import datetime, timedelta

class CostDashboardUpdater:
    def __init__(self, project_name, environment):
        self.project_name = project_name
        self.environment = environment
        self.cloudwatch = boto3.client('cloudwatch')
        self.ce_client = boto3.client('ce')
    
    def create_cost_dashboard(self):
        """ã‚³ã‚¹ãƒˆç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆ/æ›´æ–°"""
        dashboard_name = f"{self.project_name}-{self.environment}-cost-monitoring"
        
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å®šç¾©
        dashboard_body = {
            "widgets": [
                {
                    "type": "metric",
                    "x": 0,
                    "y": 0,
                    "width": 12,
                    "height": 6,
                    "properties": {
                        "metrics": [
                            ["AWS/Billing", "EstimatedCharges", "Currency", "USD"]
                        ],
                        "period": 86400,
                        "stat": "Maximum",
                        "region": "us-east-1",
                        "title": "Daily Estimated Charges"
                    }
                },
                {
                    "type": "metric",
                    "x": 0,
                    "y": 6,
                    "width": 12,
                    "height": 6,
                    "properties": {
                        "metrics": [
                            ["AWS/EC2", "CPUUtilization", "AutoScalingGroupName", f"{self.project_name}-{self.environment}-asg"]
                        ],
                        "period": 300,
                        "stat": "Average",
                        "region": "us-east-1",
                        "title": "EC2 CPU Utilization"
                    }
                }
            ]
        }
        
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’ä½œæˆ/æ›´æ–°
        response = self.cloudwatch.put_dashboard(
            DashboardName=dashboard_name,
            DashboardBody=json.dumps(dashboard_body)
        )
        
        return response
    
    def update_cost_widgets(self):
        """ã‚³ã‚¹ãƒˆã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°"""
        # éå»30æ—¥é–“ã®ã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        
        cost_data = self.ce_client.get_cost_and_usage(
            TimePeriod={
                'Start': start_date.strftime('%Y-%m-%d'),
                'End': end_date.strftime('%Y-%m-%d')
            },
            Granularity='DAILY',
            Metrics=['BlendedCost'],
            GroupBy=[
                {
                    'Type': 'DIMENSION',
                    'Key': 'SERVICE'
                }
            ]
        )
        
        return cost_data

if __name__ == "__main__":
    updater = CostDashboardUpdater("embedding-batch-workload", "production")
    result = updater.create_cost_dashboard()
    print(f"Dashboard updated: {result}")
```

---

## ğŸ“Š ã‚³ã‚¹ãƒˆæœ€é©åŒ–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### æ—¥æ¬¡ãƒã‚§ãƒƒã‚¯
- [ ] æ—¥æ¬¡ã‚³ã‚¹ãƒˆä½¿ç”¨é‡ã®ç¢ºèª
- [ ] äºˆç®—ã‚¢ãƒ©ãƒ¼ãƒˆã®ç¢ºèª
- [ ] Spotä¸­æ–­ã®ç¢ºèª
- [ ] æœªä½¿ç”¨ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª

### é€±æ¬¡ãƒã‚§ãƒƒã‚¯
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã®åˆ†æ
- [ ] Spotä¾¡æ ¼ãƒˆãƒ¬ãƒ³ãƒ‰ã®ç¢ºèª
- [ ] ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—æœ€é©åŒ–ã®æ¤œè¨
- [ ] ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ã®ç¢ºèª

### æœˆæ¬¡ãƒã‚§ãƒƒã‚¯
- [ ] åŒ…æ‹¬çš„ãªã‚³ã‚¹ãƒˆåˆ†æ
- [ ] äºˆç®—ã®è¦‹ç›´ã—
- [ ] Reserved Instance ã®æ¤œè¨
- [ ] Savings Plans ã®æ¤œè¨
- [ ] ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æœ€é©åŒ–ã®æ¤œè¨

---

ã“ã®ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã‚¬ã‚¤ãƒ‰ã‚’æ´»ç”¨ã—ã¦ã€åŠ¹ç‡çš„ã§çµŒæ¸ˆçš„ãªã‚·ã‚¹ãƒ†ãƒ é‹ç”¨ã‚’å®Ÿç¾ã—ã¦ãã ã•ã„ã€‚ç¶™ç¶šçš„ãªç›£è¦–ã¨æœ€é©åŒ–ã«ã‚ˆã‚Šã€å¤§å¹…ãªã‚³ã‚¹ãƒˆå‰Šæ¸›ãŒå¯èƒ½ã§ã™ã€‚