# ğŸ“Š Operations & Monitoring Guide

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Embedding Batch Workload Template ã®é‹ç”¨ç›£è¦–ã€ä¿å®ˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

- [ğŸ” ç›£è¦–æˆ¦ç•¥](#-ç›£è¦–æˆ¦ç•¥)
- [ğŸ“ˆ CloudWatch ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰](#-cloudwatch-ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰)
- [ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š](#-ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š)
- [ğŸ”§ æ—¥å¸¸é‹ç”¨](#-æ—¥å¸¸é‹ç”¨)
- [ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–](#-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–)
- [ğŸ› ï¸ ä¿å®ˆä½œæ¥­](#ï¸-ä¿å®ˆä½œæ¥­)

---

## ğŸ” ç›£è¦–æˆ¦ç•¥

### ç›£è¦–ãƒ¬ãƒ™ãƒ«

#### 1. ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ç›£è¦–
- **AWS Batch**: Compute Environmentã€Job Queueã€Jobå®Ÿè¡ŒçŠ¶æ³
- **Amazon FSx**: ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®å¯ç”¨æ€§ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **VPC**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—
- **IAM**: ãƒ­ãƒ¼ãƒ«ã¨ãƒãƒªã‚·ãƒ¼ã®ä½¿ç”¨çŠ¶æ³

#### 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç›£è¦–
- **ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ**: æˆåŠŸç‡ã€å®Ÿè¡Œæ™‚é–“ã€ã‚¨ãƒ©ãƒ¼ç‡
- **Bedrock API**: APIå‘¼ã³å‡ºã—å›æ•°ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã€ã‚¨ãƒ©ãƒ¼ç‡
- **ãƒ‡ãƒ¼ã‚¿å‡¦ç†**: å‡¦ç†æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã€åŸ‹ã‚è¾¼ã¿ç”Ÿæˆæ•°

#### 3. ãƒ“ã‚¸ãƒã‚¹ç›£è¦–
- **ã‚³ã‚¹ãƒˆ**: ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨æ–™é‡‘ã€äºˆç®—ã‚¢ãƒ©ãƒ¼ãƒˆ
- **SLA**: å‡¦ç†æ™‚é–“ã€å¯ç”¨æ€§ã€å“è³ªæŒ‡æ¨™
- **å®¹é‡**: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ã€å‡¦ç†èƒ½åŠ›

### ç›£è¦–ãƒ„ãƒ¼ãƒ«æ§‹æˆ

```mermaid
graph TB
    A[CloudWatch Metrics] --> B[CloudWatch Dashboard]
    A --> C[CloudWatch Alarms]
    C --> D[SNS Notifications]
    D --> E[Email/Slack/PagerDuty]
    
    F[CloudWatch Logs] --> G[Log Insights]
    F --> H[Log Streams]
    
    I[AWS X-Ray] --> J[Distributed Tracing]
    K[AWS Cost Explorer] --> L[Cost Analysis]
```

---

## ğŸ“ˆ CloudWatch ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

### 1. ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ

#### CDKå®Ÿè£…
```typescript
import { Dashboard, GraphWidget, Metric, SingleValueWidget } from 'aws-cdk-lib/aws-cloudwatch';

export class MonitoringDashboard extends Construct {
  constructor(scope: Construct, id: string, props: MonitoringDashboardProps) {
    super(scope, id);

    const dashboard = new Dashboard(this, 'EmbeddingWorkloadDashboard', {
      dashboardName: `${props.projectName}-${props.environment}-monitoring`,
      defaultInterval: Duration.minutes(5)
    });

    // Batch ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    const batchMetrics = this.createBatchMetrics(props);
    dashboard.addWidgets(...batchMetrics);

    // FSx ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    const fsxMetrics = this.createFsxMetrics(props);
    dashboard.addWidgets(...fsxMetrics);

    // ã‚³ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹
    const costMetrics = this.createCostMetrics(props);
    dashboard.addWidgets(...costMetrics);
  }

  private createBatchMetrics(props: MonitoringDashboardProps) {
    return [
      new SingleValueWidget({
        title: 'Running Jobs',
        metrics: [
          new Metric({
            namespace: 'AWS/Batch',
            metricName: 'RunningJobs',
            dimensionsMap: {
              JobQueue: props.jobQueueName
            }
          })
        ],
        width: 6,
        height: 6
      }),
      new GraphWidget({
        title: 'Job Execution Trends',
        left: [
          new Metric({
            namespace: 'AWS/Batch',
            metricName: 'SubmittedJobs',
            dimensionsMap: { JobQueue: props.jobQueueName }
          }),
          new Metric({
            namespace: 'AWS/Batch',
            metricName: 'RunnableJobs',
            dimensionsMap: { JobQueue: props.jobQueueName }
          }),
          new Metric({
            namespace: 'AWS/Batch',
            metricName: 'RunningJobs',
            dimensionsMap: { JobQueue: props.jobQueueName }
          })
        ],
        width: 12,
        height: 6
      })
    ];
  }
}
```

#### CloudFormationå®Ÿè£…
```yaml
MonitoringDashboard:
  Type: AWS::CloudWatch::Dashboard
  Properties:
    DashboardName: !Sub "${ProjectName}-${Environment}-monitoring"
    DashboardBody: !Sub |
      {
        "widgets": [
          {
            "type": "metric",
            "x": 0,
            "y": 0,
            "width": 12,
            "height": 6,
            "properties": {
              "metrics": [
                [ "AWS/Batch", "SubmittedJobs", "JobQueue", "${BatchJobQueue}" ],
                [ ".", "RunnableJobs", ".", "." ],
                [ ".", "RunningJobs", ".", "." ],
                [ ".", "CompletedJobs", ".", "." ],
                [ ".", "FailedJobs", ".", "." ]
              ],
              "period": 300,
              "stat": "Sum",
              "region": "${AWS::Region}",
              "title": "Batch Job Status"
            }
          },
          {
            "type": "metric",
            "x": 0,
            "y": 6,
            "width": 12,
            "height": 6,
            "properties": {
              "metrics": [
                [ "AWS/FSx", "DataReadBytes", "FileSystemId", "${FsxFileSystemId}" ],
                [ ".", "DataWriteBytes", ".", "." ]
              ],
              "period": 300,
              "stat": "Sum",
              "region": "${AWS::Region}",
              "title": "FSx Data Transfer"
            }
          }
        ]
      }
```

### 2. ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### Batch ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```bash
# ä¸»è¦ãªBatchãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¢ºèª
aws cloudwatch get-metric-statistics \
  --namespace AWS/Batch \
  --metric-name RunningJobs \
  --dimensions Name=JobQueue,Value=embedding-batch-job-queue \
  --start-time $(date -d '1 hour ago' --iso-8601) \
  --end-time $(date --iso-8601) \
  --period 300 \
  --statistics Sum,Average,Maximum
```

**ç›£è¦–ã™ã¹ããƒ¡ãƒˆãƒªã‚¯ã‚¹:**
- `SubmittedJobs`: æŠ•å…¥ã•ã‚ŒãŸã‚¸ãƒ§ãƒ–æ•°
- `RunnableJobs`: å®Ÿè¡Œå¾…ã¡ã‚¸ãƒ§ãƒ–æ•°
- `RunningJobs`: å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–æ•°
- `CompletedJobs`: å®Œäº†ã‚¸ãƒ§ãƒ–æ•°
- `FailedJobs`: å¤±æ•—ã‚¸ãƒ§ãƒ–æ•°

#### FSx ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```bash
# FSxãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
aws cloudwatch get-metric-statistics \
  --namespace AWS/FSx \
  --metric-name TotalIOTime \
  --dimensions Name=FileSystemId,Value=fs-12345678 \
  --start-time $(date -d '1 hour ago' --iso-8601) \
  --end-time $(date --iso-8601) \
  --period 300 \
  --statistics Average
```

**ç›£è¦–ã™ã¹ããƒ¡ãƒˆãƒªã‚¯ã‚¹:**
- `DataReadBytes`: èª­ã¿å–ã‚Šãƒ‡ãƒ¼ã‚¿é‡
- `DataWriteBytes`: æ›¸ãè¾¼ã¿ãƒ‡ãƒ¼ã‚¿é‡
- `TotalIOTime`: I/Oå¿œç­”æ™‚é–“
- `ClientConnections`: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶šæ•°

#### EC2 ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```bash
# Compute Environment ã®EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
aws cloudwatch get-metric-statistics \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --dimensions Name=AutoScalingGroupName,Value=batch-compute-env-asg \
  --start-time $(date -d '1 hour ago' --iso-8601) \
  --end-time $(date --iso-8601) \
  --period 300 \
  --statistics Average,Maximum
```

---

## ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

### 1. é‡è¦åº¦åˆ¥ã‚¢ãƒ©ãƒ¼ãƒˆ

#### Critical (å³åº§å¯¾å¿œ)
```yaml
# ã‚¸ãƒ§ãƒ–å¤±æ•—ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
JobFailureRateAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: !Sub "${ProjectName}-${Environment}-job-failure-rate-critical"
    AlarmDescription: "High job failure rate detected"
    MetricName: FailedJobs
    Namespace: AWS/Batch
    Statistic: Sum
    Period: 300
    EvaluationPeriods: 2
    Threshold: 5
    ComparisonOperator: GreaterThanThreshold
    Dimensions:
      - Name: JobQueue
        Value: !Ref BatchJobQueue
    AlarmActions:
      - !Ref CriticalAlarmTopic
    TreatMissingData: notBreaching

# FSxå¯ç”¨æ€§ã‚¢ãƒ©ãƒ¼ãƒˆ
FsxAvailabilityAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: !Sub "${ProjectName}-${Environment}-fsx-unavailable"
    AlarmDescription: "FSx file system is unavailable"
    MetricName: ClientConnections
    Namespace: AWS/FSx
    Statistic: Sum
    Period: 300
    EvaluationPeriods: 3
    Threshold: 1
    ComparisonOperator: LessThanThreshold
    Dimensions:
      - Name: FileSystemId
        Value: !Ref FsxFileSystemId
    AlarmActions:
      - !Ref CriticalAlarmTopic
```

#### Warning (ç›£è¦–å¼·åŒ–)
```yaml
# CPUä½¿ç”¨ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
HighCpuUtilizationAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: !Sub "${ProjectName}-${Environment}-high-cpu-utilization"
    AlarmDescription: "High CPU utilization in compute environment"
    MetricName: CPUUtilization
    Namespace: AWS/EC2
    Statistic: Average
    Period: 300
    EvaluationPeriods: 3
    Threshold: 80
    ComparisonOperator: GreaterThanThreshold
    AlarmActions:
      - !Ref WarningAlarmTopic

# ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼æ»ç•™ã‚¢ãƒ©ãƒ¼ãƒˆ
JobQueueBacklogAlarm:
  Type: AWS::CloudWatch::Alarm
  Properties:
    AlarmName: !Sub "${ProjectName}-${Environment}-job-queue-backlog"
    AlarmDescription: "Jobs are backing up in the queue"
    MetricName: RunnableJobs
    Namespace: AWS/Batch
    Statistic: Average
    Period: 600
    EvaluationPeriods: 2
    Threshold: 50
    ComparisonOperator: GreaterThanThreshold
    Dimensions:
      - Name: JobQueue
        Value: !Ref BatchJobQueue
    AlarmActions:
      - !Ref WarningAlarmTopic
```

### 2. é€šçŸ¥è¨­å®š

#### SNS Topicè¨­å®š
```yaml
CriticalAlarmTopic:
  Type: AWS::SNS::Topic
  Properties:
    TopicName: !Sub "${ProjectName}-${Environment}-critical-alarms"
    DisplayName: "Critical Alarms"

CriticalAlarmSubscription:
  Type: AWS::SNS::Subscription
  Properties:
    Protocol: email
    TopicArn: !Ref CriticalAlarmTopic
    Endpoint: !Ref AlertEmail

# Slackçµ±åˆ (LambdaçµŒç”±)
SlackNotificationFunction:
  Type: AWS::Lambda::Function
  Properties:
    FunctionName: !Sub "${ProjectName}-${Environment}-slack-notification"
    Runtime: python3.9
    Handler: index.lambda_handler
    Code:
      ZipFile: |
        import json
        import urllib3
        import os
        
        def lambda_handler(event, context):
            webhook_url = os.environ['SLACK_WEBHOOK_URL']
            
            message = {
                "text": f"ğŸš¨ Alert: {event['Records'][0]['Sns']['Subject']}",
                "attachments": [{
                    "color": "danger",
                    "fields": [{
                        "title": "Message",
                        "value": event['Records'][0]['Sns']['Message'],
                        "short": False
                    }]
                }]
            }
            
            http = urllib3.PoolManager()
            response = http.request('POST', webhook_url,
                                  body=json.dumps(message),
                                  headers={'Content-Type': 'application/json'})
            
            return {'statusCode': 200}
    Environment:
      Variables:
        SLACK_WEBHOOK_URL: !Ref SlackWebhookUrl
```

---

## ğŸ”§ æ—¥å¸¸é‹ç”¨

### 1. æ—¥æ¬¡ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash
# daily-health-check.sh

PROJECT_NAME="embedding-batch-workload"
ENVIRONMENT="production"
DATE=$(date +%Y-%m-%d)

echo "=== Daily Health Check - $DATE ==="

# 1. Batchç’°å¢ƒã®çŠ¶æ…‹ç¢ºèª
echo "1. Checking Batch Environment..."
aws batch describe-compute-environments \
  --compute-environments "${PROJECT_NAME}-${ENVIRONMENT}-compute-env" \
  --query 'computeEnvironments[0].{Status:status,State:state,RunningCapacity:computeResources.desiredvCpus}'

# 2. ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ç¢ºèª
echo "2. Checking Job Queue..."
aws batch describe-job-queues \
  --job-queues "${PROJECT_NAME}-${ENVIRONMENT}-job-queue" \
  --query 'jobQueues[0].{State:state,Priority:priority}'

# 3. éå»24æ™‚é–“ã®ã‚¸ãƒ§ãƒ–å®Ÿè¡ŒçŠ¶æ³
echo "3. Job Execution Summary (Last 24h)..."
aws batch list-jobs \
  --job-queue "${PROJECT_NAME}-${ENVIRONMENT}-job-queue" \
  --job-status SUCCEEDED \
  --query 'length(jobSummary)'

aws batch list-jobs \
  --job-queue "${PROJECT_NAME}-${ENVIRONMENT}-job-queue" \
  --job-status FAILED \
  --query 'length(jobSummary)'

# 4. FSxãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®çŠ¶æ…‹
echo "4. Checking FSx File System..."
aws fsx describe-file-systems \
  --file-system-ids "$FSX_FILE_SYSTEM_ID" \
  --query 'FileSystems[0].{Lifecycle:lifecycle,StorageCapacity:storageCapacity,ThroughputCapacity:throughputCapacity}'

# 5. ã‚³ã‚¹ãƒˆç¢ºèªï¼ˆå‰æ—¥åˆ†ï¼‰
echo "5. Cost Analysis (Yesterday)..."
YESTERDAY=$(date -d '1 day ago' +%Y-%m-%d)
aws ce get-cost-and-usage \
  --time-period Start=$YESTERDAY,End=$DATE \
  --granularity DAILY \
  --metrics BlendedCost \
  --group-by Type=DIMENSION,Key=SERVICE \
  --query 'ResultsByTime[0].Groups[?Keys[0]==`Amazon Elastic Compute Cloud - Compute`].Metrics.BlendedCost.Amount'

echo "=== Health Check Complete ==="
```

### 2. é€±æ¬¡ãƒ¬ãƒãƒ¼ãƒˆ

#### è‡ªå‹•ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
```python
#!/usr/bin/env python3
# weekly-report.py

import boto3
import json
from datetime import datetime, timedelta
import pandas as pd

class WeeklyReportGenerator:
    def __init__(self, project_name, environment):
        self.project_name = project_name
        self.environment = environment
        self.batch_client = boto3.client('batch')
        self.cloudwatch = boto3.client('cloudwatch')
        self.ce_client = boto3.client('ce')
    
    def generate_report(self):
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        
        report = {
            'period': f"{start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}",
            'job_statistics': self.get_job_statistics(start_date, end_date),
            'performance_metrics': self.get_performance_metrics(start_date, end_date),
            'cost_analysis': self.get_cost_analysis(start_date, end_date),
            'recommendations': self.generate_recommendations()
        }
        
        return report
    
    def get_job_statistics(self, start_date, end_date):
        # ã‚¸ãƒ§ãƒ–çµ±è¨ˆã®å–å¾—
        job_queue = f"{self.project_name}-{self.environment}-job-queue"
        
        succeeded_jobs = self.batch_client.list_jobs(
            jobQueue=job_queue,
            jobStatus='SUCCEEDED'
        )
        
        failed_jobs = self.batch_client.list_jobs(
            jobQueue=job_queue,
            jobStatus='FAILED'
        )
        
        return {
            'total_succeeded': len(succeeded_jobs['jobSummary']),
            'total_failed': len(failed_jobs['jobSummary']),
            'success_rate': len(succeeded_jobs['jobSummary']) / 
                          (len(succeeded_jobs['jobSummary']) + len(failed_jobs['jobSummary'])) * 100
        }
    
    def get_performance_metrics(self, start_date, end_date):
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—
        metrics = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/Batch',
            MetricName='RunningJobs',
            Dimensions=[
                {
                    'Name': 'JobQueue',
                    'Value': f"{self.project_name}-{self.environment}-job-queue"
                }
            ],
            StartTime=start_date,
            EndTime=end_date,
            Period=3600,
            Statistics=['Average', 'Maximum']
        )
        
        return {
            'avg_running_jobs': sum(d['Average'] for d in metrics['Datapoints']) / len(metrics['Datapoints']),
            'max_running_jobs': max(d['Maximum'] for d in metrics['Datapoints'])
        }
    
    def get_cost_analysis(self, start_date, end_date):
        # ã‚³ã‚¹ãƒˆåˆ†æ
        response = self.ce_client.get_cost_and_usage(
            TimePeriod={
                'Start': start_date.strftime('%Y-%m-%d'),
                'End': end_date.strftime('%Y-%m-%d')
            },
            Granularity='DAILY',
            Metrics=['BlendedCost'],
            GroupBy=[
                {
                    'Type': 'DIMENSION',
                    'Key': 'SERVICE'
                }
            ]
        )
        
        total_cost = 0
        service_costs = {}
        
        for result in response['ResultsByTime']:
            for group in result['Groups']:
                service = group['Keys'][0]
                cost = float(group['Metrics']['BlendedCost']['Amount'])
                service_costs[service] = service_costs.get(service, 0) + cost
                total_cost += cost
        
        return {
            'total_cost': total_cost,
            'service_breakdown': service_costs
        }
    
    def generate_recommendations(self):
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        recommendations = []
        
        # ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®æ¨å¥¨
        recommendations.append({
            'category': 'Cost Optimization',
            'recommendation': 'Consider using Spot instances for non-critical workloads',
            'impact': 'High',
            'effort': 'Medium'
        })
        
        return recommendations

if __name__ == "__main__":
    generator = WeeklyReportGenerator("embedding-batch-workload", "production")
    report = generator.generate_report()
    
    # ãƒ¬ãƒãƒ¼ãƒˆã‚’JSONå½¢å¼ã§å‡ºåŠ›
    print(json.dumps(report, indent=2, default=str))
```

---

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

### 1. ã‚¸ãƒ§ãƒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ

#### ã‚¸ãƒ§ãƒ–å®Ÿè¡Œæ™‚é–“ã®åˆ†æ
```bash
#!/bin/bash
# analyze-job-performance.sh

JOB_QUEUE="embedding-batch-workload-prod-job-queue"
DAYS_BACK=7

echo "=== Job Performance Analysis (Last $DAYS_BACK days) ==="

# å®Œäº†ã—ãŸã‚¸ãƒ§ãƒ–ã®ä¸€è¦§å–å¾—
aws batch list-jobs \
  --job-queue "$JOB_QUEUE" \
  --job-status SUCCEEDED \
  --query 'jobSummary[*].{JobId:jobId,JobName:jobName,CreatedAt:createdAt,StartedAt:startedAt,StoppedAt:stoppedAt}' \
  --output table

# å¹³å‡å®Ÿè¡Œæ™‚é–“ã®è¨ˆç®—
echo "Calculating average execution times..."

# ã‚¸ãƒ§ãƒ–è©³ç´°ã®å–å¾—ã¨åˆ†æ
aws batch list-jobs \
  --job-queue "$JOB_QUEUE" \
  --job-status SUCCEEDED \
  --query 'jobSummary[*].jobId' \
  --output text | \
while read job_id; do
  aws batch describe-jobs --jobs "$job_id" \
    --query 'jobs[0].{JobName:jobName,StartedAt:startedAt,StoppedAt:stoppedAt}' \
    --output json
done | jq -s '
  map(select(.StartedAt and .StoppedAt)) |
  map({
    JobName: .JobName,
    Duration: ((.StoppedAt | tonumber) - (.StartedAt | tonumber))
  }) |
  group_by(.JobName) |
  map({
    JobType: .[0].JobName,
    Count: length,
    AvgDuration: (map(.Duration) | add / length),
    MinDuration: (map(.Duration) | min),
    MaxDuration: (map(.Duration) | max)
  })
'
```

### 2. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ç›£è¦–

#### CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®ç¢ºèª
```bash
#!/bin/bash
# resource-utilization-check.sh

COMPUTE_ENV="embedding-batch-workload-prod-compute-env"
START_TIME=$(date -d '1 hour ago' --iso-8601)
END_TIME=$(date --iso-8601)

echo "=== Resource Utilization Check ==="

# CPUä½¿ç”¨ç‡
echo "CPU Utilization:"
aws cloudwatch get-metric-statistics \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --dimensions Name=AutoScalingGroupName,Value="$COMPUTE_ENV-asg" \
  --start-time "$START_TIME" \
  --end-time "$END_TIME" \
  --period 300 \
  --statistics Average,Maximum \
  --query 'Datapoints[*].{Time:Timestamp,Avg:Average,Max:Maximum}' \
  --output table

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ï¼ˆCloudWatch AgentãŒå¿…è¦ï¼‰
echo "Memory Utilization:"
aws cloudwatch get-metric-statistics \
  --namespace CWAgent \
  --metric-name mem_used_percent \
  --dimensions Name=AutoScalingGroupName,Value="$COMPUTE_ENV-asg" \
  --start-time "$START_TIME" \
  --end-time "$END_TIME" \
  --period 300 \
  --statistics Average,Maximum \
  --query 'Datapoints[*].{Time:Timestamp,Avg:Average,Max:Maximum}' \
  --output table
```

### 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

#### FSxæ¥ç¶šæ€§èƒ½ã®ç›£è¦–
```bash
#!/bin/bash
# fsx-performance-check.sh

FSX_FILE_SYSTEM_ID="fs-12345678"
START_TIME=$(date -d '1 hour ago' --iso-8601)
END_TIME=$(date --iso-8601)

echo "=== FSx Performance Check ==="

# ãƒ‡ãƒ¼ã‚¿è»¢é€é‡
echo "Data Transfer:"
aws cloudwatch get-metric-statistics \
  --namespace AWS/FSx \
  --metric-name DataReadBytes \
  --dimensions Name=FileSystemId,Value="$FSX_FILE_SYSTEM_ID" \
  --start-time "$START_TIME" \
  --end-time "$END_TIME" \
  --period 300 \
  --statistics Sum \
  --query 'Datapoints[*].{Time:Timestamp,ReadBytes:Sum}' \
  --output table

aws cloudwatch get-metric-statistics \
  --namespace AWS/FSx \
  --metric-name DataWriteBytes \
  --dimensions Name=FileSystemId,Value="$FSX_FILE_SYSTEM_ID" \
  --start-time "$START_TIME" \
  --end-time "$END_TIME" \
  --period 300 \
  --statistics Sum \
  --query 'Datapoints[*].{Time:Timestamp,WriteBytes:Sum}' \
  --output table

# I/Oå¿œç­”æ™‚é–“
echo "I/O Response Time:"
aws cloudwatch get-metric-statistics \
  --namespace AWS/FSx \
  --metric-name TotalIOTime \
  --dimensions Name=FileSystemId,Value="$FSX_FILE_SYSTEM_ID" \
  --start-time "$START_TIME" \
  --end-time "$END_TIME" \
  --period 300 \
  --statistics Average \
  --query 'Datapoints[*].{Time:Timestamp,IOTime:Average}' \
  --output table
```

---

## ğŸ› ï¸ ä¿å®ˆä½œæ¥­

### 1. å®šæœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

#### æœˆæ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash
# monthly-maintenance.sh

PROJECT_NAME="embedding-batch-workload"
ENVIRONMENT="production"
BACKUP_DATE=$(date +%Y-%m-%d)

echo "=== Monthly Maintenance - $BACKUP_DATE ==="

# 1. ãƒ­ã‚°ã®åœ§ç¸®ã¨ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
echo "1. Archiving old logs..."
aws logs create-export-task \
  --log-group-name "/aws/batch/job" \
  --from $(date -d '30 days ago' +%s)000 \
  --to $(date -d '1 day ago' +%s)000 \
  --destination "${PROJECT_NAME}-${ENVIRONMENT}-logs-archive" \
  --destination-prefix "monthly-archive/$BACKUP_DATE/"

# 2. ä¸è¦ãªã‚¸ãƒ§ãƒ–å±¥æ­´ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
echo "2. Cleaning up old job history..."
# 30æ—¥ä»¥ä¸Šå‰ã®å®Œäº†ã‚¸ãƒ§ãƒ–ã‚’å‰Šé™¤ï¼ˆå®Ÿè£…ã¯ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´ï¼‰

# 3. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ãƒƒãƒã®ç¢ºèª
echo "3. Checking for security updates..."
# AMIã®æ›´æ–°ç¢ºèª
aws ec2 describe-images \
  --owners amazon \
  --filters "Name=name,Values=amzn2-ami-ecs-hvm-*" \
  --query 'Images | sort_by(@, &CreationDate) | [-1].{ImageId:ImageId,Name:Name,CreationDate:CreationDate}'

# 4. ã‚³ã‚¹ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ
echo "4. Generating cost analysis..."
python3 monthly-cost-analysis.py

# 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®æ¨å¥¨
echo "5. Performance optimization recommendations..."
python3 performance-recommendations.py

echo "=== Monthly Maintenance Complete ==="
```

### 2. å®¹é‡ç®¡ç†

#### ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡ç›£è¦–
```bash
#!/bin/bash
# storage-monitoring.sh

FSX_FILE_SYSTEM_ID="fs-12345678"
S3_BUCKET="embedding-batch-workload-prod-data"

echo "=== Storage Usage Monitoring ==="

# FSxä½¿ç”¨é‡
echo "FSx Storage Usage:"
aws fsx describe-file-systems \
  --file-system-ids "$FSX_FILE_SYSTEM_ID" \
  --query 'FileSystems[0].{StorageCapacity:storageCapacity,StorageType:storageType}'

# S3ä½¿ç”¨é‡
echo "S3 Storage Usage:"
aws cloudwatch get-metric-statistics \
  --namespace AWS/S3 \
  --metric-name BucketSizeBytes \
  --dimensions Name=BucketName,Value="$S3_BUCKET" Name=StorageType,Value=StandardStorage \
  --start-time $(date -d '1 day ago' --iso-8601) \
  --end-time $(date --iso-8601) \
  --period 86400 \
  --statistics Average \
  --query 'Datapoints[0].Average'

# DynamoDBä½¿ç”¨é‡
echo "DynamoDB Table Size:"
aws dynamodb describe-table \
  --table-name "embedding-batch-workload-prod-metadata" \
  --query 'Table.{TableSizeBytes:tableSizeBytes,ItemCount:itemCount}'
```

### 3. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»

#### å®šæœŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
```bash
#!/bin/bash
# security-audit.sh

PROJECT_NAME="embedding-batch-workload"
ENVIRONMENT="production"

echo "=== Security Audit - $(date) ==="

# 1. IAMãƒ­ãƒ¼ãƒ«ã¨ãƒãƒªã‚·ãƒ¼ã®ç¢ºèª
echo "1. Checking IAM roles and policies..."
aws iam list-roles \
  --query "Roles[?contains(RoleName, '$PROJECT_NAME-$ENVIRONMENT')].{RoleName:RoleName,CreateDate:CreateDate}" \
  --output table

# 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ã®ç¢ºèª
echo "2. Checking security groups..."
aws ec2 describe-security-groups \
  --filters "Name=group-name,Values=*$PROJECT_NAME-$ENVIRONMENT*" \
  --query 'SecurityGroups[*].{GroupId:GroupId,GroupName:GroupName,IpPermissions:IpPermissions}' \
  --output table

# 3. æš—å·åŒ–è¨­å®šã®ç¢ºèª
echo "3. Checking encryption settings..."
# S3ãƒã‚±ãƒƒãƒˆã®æš—å·åŒ–
aws s3api get-bucket-encryption \
  --bucket "$PROJECT_NAME-$ENVIRONMENT-data"

# DynamoDBã®æš—å·åŒ–
aws dynamodb describe-table \
  --table-name "$PROJECT_NAME-$ENVIRONMENT-metadata" \
  --query 'Table.SSEDescription'

# 4. VPCãƒ•ãƒ­ãƒ¼ãƒ­ã‚°ã®ç¢ºèª
echo "4. Checking VPC Flow Logs..."
aws ec2 describe-flow-logs \
  --query 'FlowLogs[*].{FlowLogId:FlowLogId,ResourceId:ResourceId,FlowLogStatus:FlowLogStatus}'

# 5. CloudTrailã®ç¢ºèª
echo "5. Checking CloudTrail..."
aws cloudtrail describe-trails \
  --query 'trailList[*].{Name:Name,S3BucketName:S3BucketName,IncludeGlobalServiceEvents:IncludeGlobalServiceEvents}'

echo "=== Security Audit Complete ==="
```

### 4. ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆ

#### DR ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash
# dr-test.sh

PROJECT_NAME="embedding-batch-workload"
ENVIRONMENT="production"
DR_REGION="us-west-2"

echo "=== Disaster Recovery Test - $(date) ==="

# 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ç¢ºèª
echo "1. Verifying backups..."
aws s3 ls s3://$PROJECT_NAME-$ENVIRONMENT-backup/ --recursive --human-readable

# 2. DRãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã§ã®ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
echo "2. Checking DR region resources..."
aws fsx describe-file-systems \
  --region "$DR_REGION" \
  --query 'FileSystems[?contains(Tags[?Key==`Project`].Value, `'$PROJECT_NAME'`)].{FileSystemId:FileSystemId,Lifecycle:Lifecycle}'

# 3. ãƒ‡ãƒ¼ã‚¿åŒæœŸã®ç¢ºèª
echo "3. Verifying data synchronization..."
# S3 Cross-Region Replicationã®çŠ¶æ…‹ç¢ºèª
aws s3api get-bucket-replication \
  --bucket "$PROJECT_NAME-$ENVIRONMENT-data"

# 4. å¾©æ—§æ‰‹é †ã®ãƒ†ã‚¹ãƒˆï¼ˆéç ´å£Šçš„ï¼‰
echo "4. Testing recovery procedures (dry-run)..."
# CloudFormationãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®æ¤œè¨¼
aws cloudformation validate-template \
  --template-body file://dr-recovery-template.yaml \
  --region "$DR_REGION"

echo "=== DR Test Complete ==="
```

---

## ğŸ“‹ é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### æ—¥æ¬¡ãƒã‚§ãƒƒã‚¯
- [ ] Batchç’°å¢ƒã®çŠ¶æ…‹ç¢ºèª
- [ ] ã‚¸ãƒ§ãƒ–å®Ÿè¡ŒçŠ¶æ³ã®ç¢ºèª
- [ ] ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®ç¢ºèª
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ã®ç¢ºèª
- [ ] ã‚³ã‚¹ãƒˆä½¿ç”¨é‡ã®ç¢ºèª

### é€±æ¬¡ãƒã‚§ãƒƒã‚¯
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç¢ºèª
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ©ãƒ¼ãƒˆã®ç¢ºèª
- [ ] å®¹é‡ä½¿ç”¨é‡ã®ç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ³ã®ç¢ºèª

### æœˆæ¬¡ãƒã‚§ãƒƒã‚¯
- [ ] åŒ…æ‹¬çš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
- [ ] ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®æ¤œè¨
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å®Ÿæ–½
- [ ] ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆã®å®Ÿæ–½
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ›´æ–°

---

ã“ã®é‹ç”¨ç›£è¦–ã‚¬ã‚¤ãƒ‰ã‚’å‚è€ƒã«ã€ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šç¨¼åƒã¨æœ€é©åŒ–ã‚’å›³ã£ã¦ãã ã•ã„ã€‚å®šæœŸçš„ãªç›£è¦–ã¨ä¿å®ˆã«ã‚ˆã‚Šã€é«˜ã„å¯ç”¨æ€§ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã§ãã¾ã™ã€‚