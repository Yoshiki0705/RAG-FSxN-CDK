/**
 * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãƒ†ã‚¹ãƒˆ
 * 
 * AIå¿œç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ©Ÿèƒ½ã‚’åŒ…æ‹¬çš„ã«ãƒ†ã‚¹ãƒˆ
 * - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºãƒ†ã‚¹ãƒˆ
 * - å¿œç­”æ™‚é–“æ¸¬å®š
 * - ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­æ–­ãƒ†ã‚¹ãƒˆ
 * - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
 * - è¤‡æ•°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ
 * 
 * @version 1.0.0
 * @author NetApp Japan Technology Team
 */

import { BedrockRuntimeClient, InvokeModelWithResponseStreamCommand } from '@aws-sdk/client-bedrock-runtime';
import { fromIni } from '@aws-sdk/credential-providers';
import { TestResult, TestConfiguration } from '../types/test-types';

/**
 * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹
 */
export class StreamingResponseTests {
  private client: BedrockRuntimeClient;
  private config: TestConfiguration;
  private testResults: TestResult[] = [];

  constructor(config: TestConfiguration) {
    this.config = config;
    this.client = new BedrockRuntimeClient({
      region: config.ai.bedrockRegion,
      credentials: fromIni({ profile: process.env.AWS_PROFILE || 'user01' })
    });
  }

  /**
   * å…¨ã¦ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
   */
  async runAllTests(): Promise<TestResult[]> {
    console.log('ğŸŒŠ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãƒ†ã‚¹ãƒˆé–‹å§‹');
    this.testResults = [];

    const tests = [
      { name: 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºãƒ†ã‚¹ãƒˆ', method: this.testRealTimeDisplay.bind(this) },
      { name: 'å¿œç­”æ™‚é–“æ¸¬å®šãƒ†ã‚¹ãƒˆ', method: this.testResponseTiming.bind(this) },
      { name: 'ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­æ–­ãƒ†ã‚¹ãƒˆ', method: this.testStreamInterruption.bind(this) },
      { name: 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ', method: this.testStreamingPerformance.bind(this) },
      { name: 'è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ', method: this.testMultiModelStreaming.bind(this) },
      { name: 'ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆ', method: this.testStreamingErrorHandling.bind(this) },
      { name: 'ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ', method: this.testChunkSizeVariation.bind(this) },
      { name: 'é•·æ™‚é–“ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ', method: this.testLongStreamingSession.bind(this) }
    ];

    for (const test of tests) {
      try {
        console.log(`  ğŸ” å®Ÿè¡Œä¸­: ${test.name}`);
        const result = await test.method();
        this.testResults.push(result);
        
        if (result.status === 'passed') {
          console.log(`  âœ… æˆåŠŸ: ${test.name}`);
        } else {
          console.log(`  âŒ å¤±æ•—: ${test.name} - ${result.error}`);
        }
      } catch (error) {
        const errorResult: TestResult = {
          testName: test.name,
          category: 'AI',
          status: 'failed',
          duration: 0,
          error: error instanceof Error ? error.message : String(error),
          timestamp: new Date(),
          priority: 'high'
        };
        this.testResults.push(errorResult);
        console.log(`  âŒ ã‚¨ãƒ©ãƒ¼: ${test.name} - ${error}`);
      }
    }

    const summary = this.generateTestSummary();
    console.log(`ğŸŒŠ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãƒ†ã‚¹ãƒˆå®Œäº†: ${summary.passed}/${summary.total} æˆåŠŸ`);
    
    return this.testResults;
  }  /**

   * ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºãƒ†ã‚¹ãƒˆ
   */
  async testRealTimeDisplay(): Promise<TestResult> {
    const startTime = Date.now();
    
    try {
      const model = this.config.ai.models.claude;
      const testPrompt = 'Amazon FSx for NetApp ONTAPã®è©³ç´°ãªæ©Ÿèƒ½ã«ã¤ã„ã¦ã€æ®µéšçš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚';

      const streamingMetrics = await this.measureStreamingMetrics(model, testPrompt);

      // 1ç§’ä»¥å†…ã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹ã®è¦ä»¶ãƒã‚§ãƒƒã‚¯
      const streamingStartsWithinOneSecond = streamingMetrics.firstChunkLatency <= 1000;
      const hasRealTimeUpdates = streamingMetrics.totalChunks > 5;
      const consistentChunkTiming = streamingMetrics.averageChunkInterval < 500;

      const success = streamingStartsWithinOneSecond && hasRealTimeUpdates && consistentChunkTiming;

      return {
        testName: 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: success ? 'passed' : 'failed',
        duration: Date.now() - startTime,
        timestamp: new Date(),
        priority: 'critical',
        details: {
          model,
          firstChunkLatency: streamingMetrics.firstChunkLatency,
          totalChunks: streamingMetrics.totalChunks,
          averageChunkInterval: streamingMetrics.averageChunkInterval,
          totalStreamingTime: streamingMetrics.totalStreamingTime,
          requirements: {
            streamingStartsWithinOneSecond,
            hasRealTimeUpdates,
            consistentChunkTiming
          }
        },
        metrics: {
          responseTime: streamingMetrics.firstChunkLatency,
          throughput: streamingMetrics.totalChunks / (streamingMetrics.totalStreamingTime / 1000)
        }
      };

    } catch (error) {
      return {
        testName: 'ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: 'failed',
        duration: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        priority: 'critical'
      };
    }
  }

  /**
   * å¿œç­”æ™‚é–“æ¸¬å®šãƒ†ã‚¹ãƒˆ
   */
  async testResponseTiming(): Promise<TestResult> {
    const startTime = Date.now();
    
    try {
      const timingTests = [
        {
          model: this.config.ai.models.claude,
          prompt: 'çŸ­ã„è³ªå•: AWSã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ',
          expectedMaxLatency: 800,
          description: 'çŸ­ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ'
        },
        {
          model: this.config.ai.models.claude,
          prompt: 'ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®æ­´å²ã€ç¾åœ¨ã®çŠ¶æ³ã€å°†æ¥ã®å±•æœ›ã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ã€Amazon Web Servicesã€Microsoft Azureã€Google Cloud Platformã®æ¯”è¼ƒã‚‚å«ã‚ã¦åŒ…æ‹¬çš„ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚',
          expectedMaxLatency: 1500,
          description: 'é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ'
        },
        {
          model: this.config.ai.models.claude,
          prompt: 'Amazon FSx for NetApp ONTAPã‚’ä½¿ç”¨ã—ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­è¨ˆã«ã¤ã„ã¦ã€æŠ€è¡“çš„ãªè©³ç´°ã‚’å«ã‚ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚',
          expectedMaxLatency: 1200,
          description: 'æŠ€è¡“çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ'
        }
      ];

      const results = [];
      for (const test of timingTests) {
        const metrics = await this.measureStreamingMetrics(test.model, test.prompt);
        
        results.push({
          description: test.description,
          model: test.model,
          firstChunkLatency: metrics.firstChunkLatency,
          expectedMaxLatency: test.expectedMaxLatency,
          meetsRequirement: metrics.firstChunkLatency <= test.expectedMaxLatency,
          totalChunks: metrics.totalChunks,
          averageChunkInterval: metrics.averageChunkInterval
        });
      }

      const allMeetRequirements = results.every(r => r.meetsRequirement);
      const averageLatency = results.reduce((sum, r) => sum + r.firstChunkLatency, 0) / results.length;

      return {
        testName: 'å¿œç­”æ™‚é–“æ¸¬å®šãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: allMeetRequirements ? 'passed' : 'failed',
        duration: Date.now() - startTime,
        timestamp: new Date(),
        priority: 'high',
        details: {
          testedScenarios: timingTests.length,
          successfulScenarios: results.filter(r => r.meetsRequirement).length,
          averageLatency,
          results
        },
        metrics: {
          responseTime: averageLatency
        }
      };

    } catch (error) {
      return {
        testName: 'å¿œç­”æ™‚é–“æ¸¬å®šãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: 'failed',
        duration: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        priority: 'high'
      };
    }
  }

  /**
   * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­æ–­ãƒ†ã‚¹ãƒˆ
   */
  async testStreamInterruption(): Promise<TestResult> {
    const startTime = Date.now();
    
    try {
      const model = this.config.ai.models.claude;
      const testPrompt = 'ã¨ã¦ã‚‚é•·ã„æŠ€è¡“æ–‡æ›¸ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚Amazon Web Servicesã®å…¨ã‚µãƒ¼ãƒ“ã‚¹ã«ã¤ã„ã¦è©³ç´°ã«èª¬æ˜ã—ã€ãã‚Œãã‚Œã®ä½¿ç”¨ä¾‹ã€æ–™é‡‘ä½“ç³»ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’å«ã‚ã¦åŒ…æ‹¬çš„ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚';

      const interruptionResults = await this.testStreamingInterruption(model, testPrompt);

      const success = interruptionResults.interruptionSuccessful && 
                     interruptionResults.cleanupCompleted && 
                     interruptionResults.noMemoryLeaks;

      return {
        testName: 'ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­æ–­ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: success ? 'passed' : 'failed',
        duration: Date.now() - startTime,
        timestamp: new Date(),
        priority: 'high',
        details: {
          model,
          interruptionSuccessful: interruptionResults.interruptionSuccessful,
          cleanupCompleted: interruptionResults.cleanupCompleted,
          noMemoryLeaks: interruptionResults.noMemoryLeaks,
          chunksBeforeInterruption: interruptionResults.chunksBeforeInterruption,
          interruptionLatency: interruptionResults.interruptionLatency
        }
      };

    } catch (error) {
      return {
        testName: 'ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­æ–­ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: 'failed',
        duration: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        priority: 'high'
      };
    }
  }

  /**
   * ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
   */
  async testStreamingPerformance(): Promise<TestResult> {
    const startTime = Date.now();
    
    try {
      const performanceTests = [
        {
          name: 'åŒæ™‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼ˆ2ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰',
          concurrentSessions: 2,
          maxLatencyIncrease: 0.3 // 30%ä»¥å†…ã®é…å»¶å¢—åŠ 
        },
        {
          name: 'åŒæ™‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼ˆ5ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰',
          concurrentSessions: 5,
          maxLatencyIncrease: 0.5 // 50%ä»¥å†…ã®é…å»¶å¢—åŠ 
        },
        {
          name: 'é•·æ™‚é–“ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°',
          longSession: true,
          maxDuration: 60000, // 60ç§’
          minThroughput: 10 // æœ€ä½10ãƒãƒ£ãƒ³ã‚¯/ç§’
        }
      ];

      const results = [];
      for (const test of performanceTests) {
        if (test.concurrentSessions) {
          const concurrentResult = await this.testConcurrentStreaming(test.concurrentSessions, test.maxLatencyIncrease);
          results.push({
            testName: test.name,
            success: concurrentResult.success,
            latencyIncrease: concurrentResult.latencyIncrease,
            maxAllowedIncrease: test.maxLatencyIncrease,
            details: concurrentResult
          });
        } else if (test.longSession) {
          const longSessionResult = await this.testLongStreamingSession(test.maxDuration, test.minThroughput);
          results.push({
            testName: test.name,
            success: longSessionResult.success,
            duration: longSessionResult.duration,
            throughput: longSessionResult.throughput,
            details: longSessionResult
          });
        }
      }

      const allSuccessful = results.every(r => r.success);

      return {
        testName: 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: allSuccessful ? 'passed' : 'failed',
        duration: Date.now() - startTime,
        timestamp: new Date(),
        priority: 'high',
        details: {
          testedScenarios: performanceTests.length,
          successfulScenarios: results.filter(r => r.success).length,
          results
        }
      };

    } catch (error) {
      return {
        testName: 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: 'failed',
        duration: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        priority: 'high'
      };
    }
  }

  /**
   * è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
   */
  async testMultiModelStreaming(): Promise<TestResult> {
    const startTime = Date.now();
    
    try {
      const models = [
        'anthropic.claude-3-haiku-20240307-v1:0',
        'anthropic.claude-3-sonnet-20240229-v1:0'
      ];

      const testPrompt = 'ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ©ç‚¹ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚';
      const results = [];

      for (const model of models) {
        try {
          const metrics = await this.measureStreamingMetrics(model, testPrompt);
          
          results.push({
            model,
            success: metrics.firstChunkLatency <= 2000 && metrics.totalChunks > 0,
            firstChunkLatency: metrics.firstChunkLatency,
            totalChunks: metrics.totalChunks,
            averageChunkInterval: metrics.averageChunkInterval,
            totalStreamingTime: metrics.totalStreamingTime
          });
        } catch (error) {
          results.push({
            model,
            success: false,
            error: error instanceof Error ? error.message : String(error)
          });
        }
      }

      const successfulModels = results.filter(r => r.success).length;
      const allSuccessful = successfulModels === models.length;

      return {
        testName: 'è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: allSuccessful ? 'passed' : 'failed',
        duration: Date.now() - startTime,
        timestamp: new Date(),
        priority: 'medium',
        details: {
          testedModels: models.length,
          successfulModels,
          results
        }
      };

    } catch (error) {
      return {
        testName: 'è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: 'failed',
        duration: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        priority: 'medium'
      };
    }
  }

  /**
   * ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆ
   */
  async testStreamingErrorHandling(): Promise<TestResult> {
    const startTime = Date.now();
    
    try {
      const errorScenarios = [
        {
          name: 'ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«ID',
          modelId: 'invalid-model-id',
          prompt: 'ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ',
          expectedError: 'ValidationException'
        },
        {
          name: 'ç©ºã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ',
          modelId: this.config.ai.models.claude,
          prompt: '',
          expectedError: 'ValidationException'
        },
        {
          name: 'éåº¦ã«é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ',
          modelId: this.config.ai.models.claude,
          prompt: 'a'.repeat(200000), // 200KB
          expectedError: 'ValidationException'
        }
      ];

      const results = [];
      for (const scenario of errorScenarios) {
        const errorResult = await this.testStreamingErrorScenario(scenario);
        results.push({
          scenario: scenario.name,
          success: errorResult.success,
          expectedError: scenario.expectedError,
          actualError: errorResult.actualError,
          properlyHandled: errorResult.properlyHandled
        });
      }

      const allProperlyHandled = results.every(r => r.properlyHandled);

      return {
        testName: 'ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: allProperlyHandled ? 'passed' : 'failed',
        duration: Date.now() - startTime,
        timestamp: new Date(),
        priority: 'high',
        details: {
          testedScenarios: errorScenarios.length,
          properlyHandledScenarios: results.filter(r => r.properlyHandled).length,
          results
        }
      };

    } catch (error) {
      return {
        testName: 'ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: 'failed',
        duration: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        priority: 'high'
      };
    }
  }

  /**
   * ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ
   */
  async testChunkSizeVariation(): Promise<TestResult> {
    const startTime = Date.now();
    
    try {
      const model = this.config.ai.models.claude;
      const testPrompts = [
        {
          type: 'çŸ­æ–‡ç”Ÿæˆ',
          prompt: 'AWSã¨ã¯ä½•ã§ã™ã‹ï¼Ÿä¸€æ–‡ã§ç­”ãˆã¦ãã ã•ã„ã€‚',
          expectedChunkRange: [1, 5]
        },
        {
          type: 'ä¸­æ–‡ç”Ÿæˆ',
          prompt: 'ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®åŸºæœ¬æ¦‚å¿µã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚',
          expectedChunkRange: [5, 20]
        },
        {
          type: 'é•·æ–‡ç”Ÿæˆ',
          prompt: 'Amazon FSx for NetApp ONTAPã®è©³ç´°ãªæŠ€è¡“ä»•æ§˜ã€ä½¿ç”¨ä¾‹ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚',
          expectedChunkRange: [20, 100]
        }
      ];

      const results = [];
      for (const test of testPrompts) {
        const metrics = await this.measureStreamingMetrics(model, test.prompt);
        const chunkSizeAnalysis = this.analyzeChunkSizes(metrics.chunkSizes);
        
        const withinExpectedRange = metrics.totalChunks >= test.expectedChunkRange[0] && 
                                   metrics.totalChunks <= test.expectedChunkRange[1];

        results.push({
          type: test.type,
          totalChunks: metrics.totalChunks,
          expectedRange: test.expectedChunkRange,
          withinExpectedRange,
          averageChunkSize: chunkSizeAnalysis.averageSize,
          chunkSizeVariation: chunkSizeAnalysis.variation,
          chunkSizeConsistency: chunkSizeAnalysis.consistency
        });
      }

      const allWithinRange = results.every(r => r.withinExpectedRange);

      return {
        testName: 'ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: allWithinRange ? 'passed' : 'failed',
        duration: Date.now() - startTime,
        timestamp: new Date(),
        priority: 'medium',
        details: {
          testedPromptTypes: testPrompts.length,
          successfulTests: results.filter(r => r.withinExpectedRange).length,
          results
        }
      };

    } catch (error) {
      return {
        testName: 'ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: 'failed',
        duration: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        priority: 'medium'
      };
    }
  }

  /**
   * é•·æ™‚é–“ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
   */
  async testLongStreamingSession(): Promise<TestResult> {
    const startTime = Date.now();
    
    try {
      const model = this.config.ai.models.claude;
      const longPrompt = 'Amazon Web Servicesã®å…¨ã‚µãƒ¼ãƒ“ã‚¹ã«ã¤ã„ã¦ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚å„ã‚µãƒ¼ãƒ“ã‚¹ã®æ©Ÿèƒ½ã€ä½¿ç”¨ä¾‹ã€æ–™é‡‘ä½“ç³»ã€ä»–ã‚µãƒ¼ãƒ“ã‚¹ã¨ã®é€£æºæ–¹æ³•ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …ã‚’å«ã‚ã¦åŒ…æ‹¬çš„ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚';

      const longSessionResult = await this.testLongStreamingSession(60000, 5); // 60ç§’ã€æœ€ä½5ãƒãƒ£ãƒ³ã‚¯/ç§’

      const success = longSessionResult.success && 
                     longSessionResult.duration <= 60000 && 
                     longSessionResult.throughput >= 5;

      return {
        testName: 'é•·æ™‚é–“ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: success ? 'passed' : 'failed',
        duration: Date.now() - startTime,
        timestamp: new Date(),
        priority: 'medium',
        details: {
          model,
          sessionDuration: longSessionResult.duration,
          totalChunks: longSessionResult.totalChunks,
          throughput: longSessionResult.throughput,
          minRequiredThroughput: 5,
          maxAllowedDuration: 60000,
          memoryUsageStable: longSessionResult.memoryUsageStable,
          noConnectionDrops: longSessionResult.noConnectionDrops
        }
      };

    } catch (error) {
      return {
        testName: 'é•·æ™‚é–“ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ',
        category: 'AI',
        status: 'failed',
        duration: Date.now() - startTime,
        error: error instanceof Error ? error.message : String(error),
        timestamp: new Date(),
        priority: 'medium'
      };
    }
  }  //
 ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰

  /**
   * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸¬å®š
   */
  private async measureStreamingMetrics(modelId: string, prompt: string): Promise<{
    firstChunkLatency: number;
    totalChunks: number;
    averageChunkInterval: number;
    totalStreamingTime: number;
    chunkSizes: number[];
    contentLength: number;
  }> {
    const command = new InvokeModelWithResponseStreamCommand({
      modelId,
      body: JSON.stringify({
        anthropic_version: 'bedrock-2023-05-31',
        max_tokens: 1000,
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ]
      }),
      contentType: 'application/json',
      accept: 'application/json'
    });

    const streamingStartTime = Date.now();
    let firstChunkTime = 0;
    let totalChunks = 0;
    let totalContent = '';
    const chunkTimes: number[] = [];
    const chunkSizes: number[] = [];

    const response = await this.client.send(command);
    
    if (response.body) {
      for await (const chunk of response.body) {
        if (chunk.chunk?.bytes) {
          const currentTime = Date.now();
          
          if (firstChunkTime === 0) {
            firstChunkTime = currentTime;
          }
          
          chunkTimes.push(currentTime);
          
          const chunkData = JSON.parse(new TextDecoder().decode(chunk.chunk.bytes));
          if (chunkData.type === 'content_block_delta' && chunkData.delta?.text) {
            const chunkText = chunkData.delta.text;
            totalContent += chunkText;
            chunkSizes.push(chunkText.length);
            totalChunks++;
          }
        }
      }
    }

    const totalStreamingTime = Date.now() - streamingStartTime;
    const firstChunkLatency = firstChunkTime - streamingStartTime;
    
    // ãƒãƒ£ãƒ³ã‚¯é–“éš”ã®è¨ˆç®—
    const chunkIntervals = [];
    for (let i = 1; i < chunkTimes.length; i++) {
      chunkIntervals.push(chunkTimes[i] - chunkTimes[i - 1]);
    }
    const averageChunkInterval = chunkIntervals.length > 0 
      ? chunkIntervals.reduce((sum, interval) => sum + interval, 0) / chunkIntervals.length 
      : 0;

    return {
      firstChunkLatency,
      totalChunks,
      averageChunkInterval,
      totalStreamingTime,
      chunkSizes,
      contentLength: totalContent.length
    };
  }

  /**
   * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­æ–­ãƒ†ã‚¹ãƒˆ
   */
  private async testStreamingInterruption(modelId: string, prompt: string): Promise<{
    interruptionSuccessful: boolean;
    cleanupCompleted: boolean;
    noMemoryLeaks: boolean;
    chunksBeforeInterruption: number;
    interruptionLatency: number;
  }> {
    const command = new InvokeModelWithResponseStreamCommand({
      modelId,
      body: JSON.stringify({
        anthropic_version: 'bedrock-2023-05-31',
        max_tokens: 2000,
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ]
      }),
      contentType: 'application/json',
      accept: 'application/json'
    });

    let chunksBeforeInterruption = 0;
    let interruptionTime = 0;
    let streamInterrupted = false;
    const initialMemory = process.memoryUsage().heapUsed;

    try {
      const response = await this.client.send(command);
      
      if (response.body) {
        for await (const chunk of response.body) {
          if (chunk.chunk?.bytes) {
            const chunkData = JSON.parse(new TextDecoder().decode(chunk.chunk.bytes));
            if (chunkData.type === 'content_block_delta' && chunkData.delta?.text) {
              chunksBeforeInterruption++;
              
              // 5ãƒãƒ£ãƒ³ã‚¯å¾Œã«ä¸­æ–­ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
              if (chunksBeforeInterruption >= 5 && !streamInterrupted) {
                interruptionTime = Date.now();
                streamInterrupted = true;
                break; // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ä¸­æ–­
              }
            }
          }
        }
      }
    } catch (error) {
      // ä¸­æ–­ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼ã¯æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ
    }

    const interruptionLatency = interruptionTime > 0 ? Date.now() - interruptionTime : 0;
    
    // ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    const finalMemory = process.memoryUsage().heapUsed;
    const memoryIncrease = finalMemory - initialMemory;
    const noMemoryLeaks = memoryIncrease < 10 * 1024 * 1024; // 10MBä»¥ä¸‹ã®å¢—åŠ 

    return {
      interruptionSuccessful: streamInterrupted,
      cleanupCompleted: true, // ç°¡æ˜“å®Ÿè£…ã§ã¯å¸¸ã«true
      noMemoryLeaks,
      chunksBeforeInterruption,
      interruptionLatency
    };
  }

  /**
   * åŒæ™‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
   */
  private async testConcurrentStreaming(concurrentSessions: number, maxLatencyIncrease: number): Promise<{
    success: boolean;
    latencyIncrease: number;
    sessions: any[];
  }> {
    const model = this.config.ai.models.claude;
    const testPrompt = 'ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®åŸºæœ¬æ¦‚å¿µã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚';

    // ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
    const baselineMetrics = await this.measureStreamingMetrics(model, testPrompt);
    const baselineLatency = baselineMetrics.firstChunkLatency;

    // åŒæ™‚ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    const sessionPromises = [];
    for (let i = 0; i < concurrentSessions; i++) {
      sessionPromises.push(this.measureStreamingMetrics(model, `${testPrompt} (ã‚»ãƒƒã‚·ãƒ§ãƒ³${i + 1})`));
    }

    const sessionResults = await Promise.all(sessionPromises);
    
    // é…å»¶å¢—åŠ ã®è¨ˆç®—
    const averageConcurrentLatency = sessionResults.reduce((sum, result) => sum + result.firstChunkLatency, 0) / sessionResults.length;
    const latencyIncrease = (averageConcurrentLatency - baselineLatency) / baselineLatency;

    const success = latencyIncrease <= maxLatencyIncrease;

    return {
      success,
      latencyIncrease,
      sessions: sessionResults.map((result, index) => ({
        sessionId: index + 1,
        firstChunkLatency: result.firstChunkLatency,
        totalChunks: result.totalChunks,
        totalStreamingTime: result.totalStreamingTime
      }))
    };
  }

  /**
   * é•·æ™‚é–“ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
   */
  private async testLongStreamingSession(maxDuration: number, minThroughput: number): Promise<{
    success: boolean;
    duration: number;
    totalChunks: number;
    throughput: number;
    memoryUsageStable: boolean;
    noConnectionDrops: boolean;
  }> {
    const model = this.config.ai.models.claude;
    const longPrompt = 'Amazon Web Servicesã®å…¨ã‚µãƒ¼ãƒ“ã‚¹ã«ã¤ã„ã¦ã€è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚å„ã‚µãƒ¼ãƒ“ã‚¹ã®æ©Ÿèƒ½ã€ä½¿ç”¨ä¾‹ã€æ–™é‡‘ä½“ç³»ã‚’å«ã‚ã¦åŒ…æ‹¬çš„ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚';

    const startTime = Date.now();
    const initialMemory = process.memoryUsage().heapUsed;
    let totalChunks = 0;
    let connectionDrops = 0;

    try {
      const metrics = await this.measureStreamingMetrics(model, longPrompt);
      totalChunks = metrics.totalChunks;
      
      const duration = Date.now() - startTime;
      const throughput = totalChunks / (duration / 1000);
      
      // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
      const finalMemory = process.memoryUsage().heapUsed;
      const memoryIncrease = finalMemory - initialMemory;
      const memoryUsageStable = memoryIncrease < 50 * 1024 * 1024; // 50MBä»¥ä¸‹ã®å¢—åŠ 

      const success = duration <= maxDuration && 
                     throughput >= minThroughput && 
                     memoryUsageStable && 
                     connectionDrops === 0;

      return {
        success,
        duration,
        totalChunks,
        throughput,
        memoryUsageStable,
        noConnectionDrops: connectionDrops === 0
      };

    } catch (error) {
      connectionDrops++;
      return {
        success: false,
        duration: Date.now() - startTime,
        totalChunks,
        throughput: 0,
        memoryUsageStable: false,
        noConnectionDrops: false
      };
    }
  }

  /**
   * ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ
   */
  private async testStreamingErrorScenario(scenario: any): Promise<{
    success: boolean;
    actualError: string;
    properlyHandled: boolean;
  }> {
    try {
      let body: any = {};
      
      if (scenario.prompt === '') {
        body = {
          anthropic_version: 'bedrock-2023-05-31',
          max_tokens: 100,
          messages: [{ role: 'user', content: '' }]
        };
      } else {
        body = {
          anthropic_version: 'bedrock-2023-05-31',
          max_tokens: 100,
          messages: [{ role: 'user', content: scenario.prompt }]
        };
      }

      const command = new InvokeModelWithResponseStreamCommand({
        modelId: scenario.modelId,
        body: JSON.stringify(body),
        contentType: 'application/json',
        accept: 'application/json'
      });

      const response = await this.client.send(command);
      
      if (response.body) {
        for await (const chunk of response.body) {
          // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒé–‹å§‹ã•ã‚ŒãŸå ´åˆã€ã‚¨ãƒ©ãƒ¼ãŒæœŸå¾…ã•ã‚Œã¦ã„ãŸãŒç™ºç”Ÿã—ãªã‹ã£ãŸ
          break;
        }
      }
      
      return {
        success: false,
        actualError: 'No error occurred',
        properlyHandled: false
      };

    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      const expectedErrorOccurred = errorMessage.includes(scenario.expectedError);
      
      return {
        success: expectedErrorOccurred,
        actualError: errorMessage,
        properlyHandled: expectedErrorOccurred
      };
    }
  }

  /**
   * ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ†æ
   */
  private analyzeChunkSizes(chunkSizes: number[]): {
    averageSize: number;
    variation: number;
    consistency: number;
  } {
    if (chunkSizes.length === 0) {
      return { averageSize: 0, variation: 0, consistency: 0 };
    }

    const averageSize = chunkSizes.reduce((sum, size) => sum + size, 0) / chunkSizes.length;
    
    // åˆ†æ•£ã®è¨ˆç®—
    const variance = chunkSizes.reduce((sum, size) => sum + Math.pow(size - averageSize, 2), 0) / chunkSizes.length;
    const variation = Math.sqrt(variance);
    
    // ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ï¼ˆå¤‰å‹•ä¿‚æ•°ã®é€†æ•°ï¼‰
    const consistency = averageSize > 0 ? 1 - (variation / averageSize) : 0;

    return {
      averageSize,
      variation,
      consistency: Math.max(0, Math.min(1, consistency))
    };
  }

  /**
   * ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ
   */
  private generateTestSummary(): { total: number; passed: number; failed: number } {
    const total = this.testResults.length;
    const passed = this.testResults.filter(r => r.status === 'passed').length;
    const failed = total - passed;
    
    return { total, passed, failed };
  }
}

export default StreamingResponseTests;